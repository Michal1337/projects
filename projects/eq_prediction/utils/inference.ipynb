{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import requests\n",
    "import tqdm\n",
    "import pickle\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_params(starttime, endtime, minlatitude, maxlatitude, minlongitude, maxlongitude):\n",
    "    params = {\n",
    "        \"format\": \"geojson\",\n",
    "        \"starttime\": starttime,\n",
    "        \"endtime\": endtime,\n",
    "        \"minlatitude\": minlatitude,\n",
    "        \"maxlatitude\": maxlatitude,\n",
    "        \"minlongitude\": minlongitude,\n",
    "        \"maxlongitude\": maxlongitude\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_params_circle(starttime, endtime, latitude, longitude, maxradiuskm):\n",
    "    params = {\n",
    "        \"format\": \"geojson\",\n",
    "        \"starttime\": starttime,\n",
    "        \"endtime\": endtime,\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"maxradiuskm\": maxradiuskm\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_earthquake_count(params):\n",
    "    url = \"https://earthquake.usgs.gov/fdsnws/event/1/count\"\n",
    "    response = requests.get(url, params=params)\n",
    "    print(\"XD\")\n",
    "    print(response)\n",
    "    return response.json()[\"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_earthquake_data(params):\n",
    "    url = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n",
    "    response = requests.get(url, params=params)\n",
    "    print(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(resp, params, errors):\n",
    "    all_eqs = []\n",
    "    try:\n",
    "        for eq in resp.json()[\"features\"]:\n",
    "            prop = list(eq[\"properties\"].values())\n",
    "            prop.extend(eq[\"geometry\"][\"coordinates\"])\n",
    "            all_eqs.append(prop)\n",
    "        cols = list(resp.json()[\"features\"][0][\"properties\"].keys())\n",
    "        cols.extend([\"longitude\", \"latitude\", \"depth\"])\n",
    "        df = pd.DataFrame(all_eqs, columns=cols)\n",
    "    except:\n",
    "        errors.append(params)\n",
    "        df = pd.DataFrame()\n",
    "    return df, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1)\n",
    "    lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2)\n",
    "    lon2 = np.radians(lon2)\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    distance = R * c\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_min_dist(df_tp, x, y):\n",
    "    df_tp[\"dist\"] = haversine_distance(x, y, df_tp[\"lat\"], df_tp[\"lon\"])\n",
    "    min_dist = df_tp[\"dist\"].min()\n",
    "    plates = df_tp[df_tp[\"dist\"] == min_dist].sort_values(\"plate\")[\"plate\"].tolist()\n",
    "    plates = \"_\".join(plates)\n",
    "    return min_dist, plates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_region_info(df, df_tp, geo_split):\n",
    "    region2plate = {}\n",
    "    region2dist = {}\n",
    "    df[\"lat_cent\"] = df[\"latitude_disc\"] + 0.5\n",
    "    df[\"lon_cent\"] = df[\"longitude_disc\"] + 0.5\n",
    "    for pos in tqdm.tqdm(df[\"pos\"].unique()):\n",
    "        x, y = pos.split(\"_\")\n",
    "        x, y = float(x), float(y)\n",
    "        dist, plate = find_min_dist(df_tp, x + 0.5, y + 0.5)\n",
    "        region2plate[pos] = plate\n",
    "        region2dist[pos] = dist\n",
    "    df[\"plate_region\"] = df[\"pos\"].map(region2plate)\n",
    "    df[\"dist_region\"] = df[\"pos\"].map(region2dist)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tectonic_info(df, df_tp):\n",
    "    coordinates = list(zip(df['latitude'], df['longitude']))\n",
    "    results = list(tqdm.tqdm(map(lambda x: find_min_dist(df_tp, x[0], x[1]), coordinates), total=len(coordinates)))\n",
    "    df[[\"dist\", \"plate\"]] = results\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_col(df, col, mapping):\n",
    "    mapping = dict(zip(mapping.iloc[:, 0], mapping.iloc[:, 1]))\n",
    "    df[col] = df[col].map(mapping)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROC_PARAMS = {\n",
    "    \"mag_low\": -1,\n",
    "    \"mag_high\": 7,\n",
    "    \"depth_low\": 2,\n",
    "    \"depth_high\": 1e8,\n",
    "    \"dist_low\": 1,\n",
    "    \"dist_high\": 1e8,\n",
    "    \"dist_region_low\": 2,\n",
    "    \"dist_region_high\": 1e8,\n",
    "    \"scale_distance\": 78.44,\n",
    "    \"scale_distance_lag\": 300,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df, PREPROC_PARAMS, scaler_dict):\n",
    "\n",
    "    scaler = scaler_dict[\"mag\"]\n",
    "    df[\"mag\"] = scaler.transform(np.clip(df[\"mag\"].values, PREPROC_PARAMS[\"mag_low\"], PREPROC_PARAMS[\"mag_high\"]).reshape(-1, 1))\n",
    "\n",
    "    scaler = scaler_dict[\"depth\"]\n",
    "    df[\"depth\"] = np.log(df[\"depth\"] + np.abs(df[\"depth\"].min()) + 1)\n",
    "    df[\"depth\"] = scaler.transform(np.clip(df[\"depth\"].values, PREPROC_PARAMS[\"depth_low\"], PREPROC_PARAMS[\"depth_high\"]).reshape(-1, 1))\n",
    "\n",
    "    scaler = scaler_dict[\"latitude_new\"]\n",
    "    df[\"latitude_new\"] = scaler.transform(df[\"latitude\"].values.reshape(-1, 1))\n",
    "\n",
    "    scaler = scaler_dict[\"longitude_new\"]\n",
    "    df[\"longitude_new\"] = scaler.transform(df[\"longitude\"].values.reshape(-1, 1))\n",
    "\n",
    "    scaler = scaler_dict[\"lat_cent\"]\n",
    "    df[\"lat_cent\"] = scaler.transform(df[\"lat_cent\"].values.reshape(-1, 1))\n",
    "\n",
    "    scaler = scaler_dict[\"lon_cent\"]\n",
    "    df[\"lon_cent\"] = scaler.transform(df[\"lon_cent\"].values.reshape(-1, 1))\n",
    "\n",
    "    scaler = scaler_dict[\"dist\"]\n",
    "    df[\"dist\"] = df[\"dist\"].astype(float)\n",
    "    df[\"dist\"] = scaler.transform(np.clip(np.log(df[\"dist\"] + 1).values.reshape(-1, 1), PREPROC_PARAMS[\"dist_low\"], PREPROC_PARAMS[\"dist_high\"]))\n",
    "\n",
    "    scaler = scaler_dict[\"dist_region\"]\n",
    "    df[\"dist_region\"] = scaler.transform(np.clip(np.log(df[\"dist_region\"] + 1).values.reshape(-1, 1), PREPROC_PARAMS[\"dist_region_low\"], PREPROC_PARAMS[\"dist_region_high\"]))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_block(df, pos, radius, block_size, PREPROC_PARAMS):\n",
    "    bins = [0, 1, 2, 3, 4, 5, 6, 7, 10, 14, 21, 30, 60, 180, 1e8]\n",
    "    lat, lon = pos.split(\"_\")\n",
    "    lat, lon = float(lat), float(lon)\n",
    "    tmp1 = df[df[\"pos\"] == pos]\n",
    "    diff = int(radius / 111) + 3\n",
    "    tmp2 = df[((df[\"latitude\"] >= lat - diff) & (df[\"latitude\"] <= lat + diff) & (df[\"longitude\"] >= lon - diff) & (df[\"longitude\"] <= lon + diff)) & (df[\"pos\"] != pos)]\n",
    "    tmp1[\"label\"] = 0\n",
    "    tmp2[\"label\"] = -1\n",
    "    tmp = pd.concat([tmp1, tmp2], axis=0)\n",
    "    tmp = tmp[tmp[\"distance\"] <= radius]\n",
    "    tmp.sort_values(by=[\"time\"], inplace=True)\n",
    "    tmp[\"diff_days\"] = (tmp[\"time\"] - tmp[\"time\"].shift(1)).dt.days\n",
    "    tmp.dropna(inplace=True)\n",
    "    tmp[\"diff_days\"] = np.digitize(tmp[\"diff_days\"], bins=bins) - 1\n",
    "    for idx in range(1, block_size):\n",
    "        tmp[\"mag_\" + str(idx)] = tmp[\"mag\"].shift(idx)\n",
    "        tmp[\"depth_\" + str(idx)] = tmp[\"depth\"].shift(idx)\n",
    "        tmp[\"latitude_new_\" + str(idx)] = tmp[\"latitude_new\"].shift(idx)\n",
    "        tmp[\"longitude_new_\" + str(idx)] = tmp[\"longitude_new\"].shift(idx)\n",
    "        tmp[\"dist_\" + str(idx)] = tmp[\"dist\"].shift(idx)\n",
    "        tmp[\"distance_\" + str(idx)] = tmp[\"distance\"].shift(idx) / PREPROC_PARAMS[\"scale_distance_lag\"]\n",
    "        tmp[\"plate_\" + str(idx)] = tmp[\"plate\"].shift(idx)\n",
    "        tmp[\"diff_days_\" + str(idx)] = tmp[\"diff_days\"].shift(idx)\n",
    "        tmp[\"magType_\" + str(idx)] = tmp[\"magType\"].shift(idx)\n",
    "    tmp = tmp[tmp[\"label\"] != -1]\n",
    "    tmp[\"distance\"] = tmp[\"distance\"] / PREPROC_PARAMS[\"scale_distance\"]\n",
    "    tmp.dropna(inplace=True)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(df, block_size, feature_order, featrues_region):\n",
    "    x_ts = df[feature_order].to_numpy().reshape(-1, block_size, len(feature_order) // block_size)\n",
    "    x_region = df[featrues_region].to_numpy().reshape(-1, len(featrues_region))\n",
    "    return x_ts, x_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_coords(X, Y, regions):\n",
    "    if X < -180 or X > 180 or Y < -90 or Y > 90:\n",
    "        return False\n",
    "    for pos in regions:\n",
    "        y, x = pos.split(\"_\")\n",
    "        y, x = float(y), float(x)\n",
    "        if X >= x and X <= x + GEO_SPLIT and Y >= y and Y <= y + GEO_SPLIT:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = pickle.load(open(\"../data/scalers_for_npys.pkl\", \"rb\"))\n",
    "regions = np.load(\"../data/regions.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Union\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def make_timeseries(\n",
    "    df: pd.DataFrame,\n",
    "    x: float,\n",
    "    y: float,\n",
    "    radius: int,\n",
    "    block_size: int,\n",
    "    features_order: List[str],\n",
    "    features_region: List[str],\n",
    "    preproc_params: Dict[str, Union[int, float, List[int]]],\n",
    "    scaler_dict: Dict[str, MinMaxScaler],\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"], format=\"mixed\")\n",
    "    df.sort_values(by=\"time\", inplace=True)\n",
    "    pos = str(y - GEO_SPLIT / 2) + \"_\" + str(x - GEO_SPLIT / 2)\n",
    "    df = preprocess_df(df, preproc_params, scaler_dict)\n",
    "    df_pos = make_block(df, pos, radius, block_size, preproc_params)\n",
    "    df_pos = df_pos.iloc[-1]\n",
    "    time = df_pos[\"time\"]\n",
    "    if dt.datetime.now() - time > dt.timedelta(days=30):\n",
    "        return None, None, None\n",
    "    x_ts, x_region = reshape(df_pos, block_size, features_order, features_region)\n",
    "    return x_ts, x_region, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df: pd.DataFrame, geo_split: int) -> pd.DataFrame:\n",
    "    df_tp = pd.read_csv(\"../data/all.csv\")\n",
    "    df_tp.drop_duplicates(inplace=True)\n",
    "\n",
    "    df = add_region_info(df, df_tp, geo_split)\n",
    "    df = add_tectonic_info(df, df_tp)\n",
    "    mapping1 = pd.read_csv(\"../data/magtype2id.csv\")\n",
    "    mapping2 = pd.read_csv(\"../data/plate2id.csv\")\n",
    "    mapping3 = pd.read_csv(\"../data/plate_region2id.csv\")\n",
    "\n",
    "    df = map_col(df, \"magType\", mapping1)\n",
    "    df = map_col(df, \"plate\", mapping2)\n",
    "    df = map_col(df, \"plate_region\", mapping3)\n",
    "\n",
    "    df[\"plate_region\"] = df[\"plate_region\"].fillna(51)\n",
    "    df[\"plate\"] = df[\"plate\"].fillna(61)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_order(features: List[str], block_size: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Creates the feature order for the time-series block. The order is defined as follows: [feature_1_lag_block_size-1, feature_2_lag_block_size-1, ..., feature_1_lag_0, feature_2_lag_0].\n",
    "\n",
    "    Parameters:\n",
    "    - features (List[str]): The features of the block.\n",
    "    - block_size (int): The size of the block.\n",
    "\n",
    "    Returns:\n",
    "    - features_order (List[str]): The feature order of the block.\n",
    "\n",
    "    Example:\n",
    "    >>> make_feature_order([\"mag\", \"depth\"], 2)\n",
    "    ['mag_1', 'depth_1', 'mag_0', 'depth_0']\n",
    "    \"\"\"\n",
    "    features_order = [\n",
    "        features[idx] + \"_\" + str(i)\n",
    "        for i in range(block_size - 1, 0, -1)\n",
    "        for idx in range(len(features))\n",
    "    ]\n",
    "    features_order = features_order + features\n",
    "    return features_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_REGION: List[str] = [\"lat_cent\", \"lon_cent\", \"dist_region\", \"plate_region\"]\n",
    "FEATURES: List[str] = [\n",
    "    \"mag\",\n",
    "    \"depth\",\n",
    "    \"latitude_new\",\n",
    "    \"longitude_new\",\n",
    "    \"dist\",\n",
    "    \"distance\",\n",
    "    \"plate\",\n",
    "    \"diff_days\",\n",
    "    \"magType\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(X, Y, start_time, end_time, min_lat, max_lat, min_lon, max_lon, geo_split, radius, block_size):\n",
    "    errors = []\n",
    "    count = get_earthquake_count(\n",
    "        make_params(start_time, end_time, min_lat, max_lat, min_lon, max_lon)\n",
    "    )\n",
    "    if count == 0:\n",
    "        print(\"Last earthquake was more than 30 days ago\")\n",
    "        return None, None, None\n",
    "    print(start_time, count)\n",
    "    resp = get_earthquake_data(\n",
    "        make_params(start_time, end_time, min_lat, max_lat, min_lon, max_lon)\n",
    "    )\n",
    "    df, errors = make_df(\n",
    "        resp,\n",
    "        make_params(start_time, end_time, min_lat, max_lat, min_lon, max_lon),\n",
    "        errors,\n",
    "    )\n",
    "    df = df[df[\"type\"] == \"earthquake\"]\n",
    "    df[\"time\"] = df[\"time\"].apply(lambda x: dt.datetime.fromtimestamp(x / 1000))\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"], format=\"mixed\")\n",
    "    df = df[[\"time\", \"longitude\", \"latitude\", \"depth\", \"mag\", \"magType\"]]\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    if len(df) == 0:\n",
    "        print(\"Last earthquake was more than 30 days ago v2\")\n",
    "        return None, None, None # No data\n",
    "    end_time = df[\"time\"].max()\n",
    "    count = 0\n",
    "    cnt = 0\n",
    "    while count < 65:\n",
    "        if cnt > 50:\n",
    "            print(\"No data\")\n",
    "            return None, None, None\n",
    "        start_time = start_time - dt.timedelta(days=360)\n",
    "        count = get_earthquake_count(\n",
    "            make_params_circle(start_time, end_time, Y, X, radius+10)\n",
    "        )\n",
    "        cnt += 1\n",
    "        print(start_time, end_time, count)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    while cnt < 20:\n",
    "        cnt += 1\n",
    "        n = count // 20000\n",
    "        dates = pd.date_range(start_time, end_time, periods= 2 if count < 20000 else n + 4)\n",
    "        for i in range(len(dates)-1):\n",
    "            print(dates[i], dates[i+1])\n",
    "            resp = get_earthquake_data(\n",
    "                    make_params_circle(\n",
    "                        dates[i].to_pydatetime(),\n",
    "                        dates[i + 1].to_pydatetime(),\n",
    "                        Y,\n",
    "                        X,\n",
    "                        radius+10,\n",
    "                    )\n",
    "                )\n",
    "            df_small, errors = make_df(\n",
    "                resp,\n",
    "                make_params_circle(\n",
    "                    dates[i].to_pydatetime(),\n",
    "                    dates[i + 1].to_pydatetime(),\n",
    "                    Y,\n",
    "                    X,\n",
    "                    radius+10,\n",
    "                ),\n",
    "                errors,\n",
    "            )\n",
    "            df_small = df_small[df_small[\"type\"] == \"earthquake\"]\n",
    "            df_small = df_small[\n",
    "                [\"time\", \"longitude\", \"latitude\", \"depth\", \"mag\", \"magType\"]\n",
    "            ]\n",
    "            df_small[\"time\"] = df_small[\"time\"].apply(\n",
    "                lambda x: dt.datetime.fromtimestamp(x / 1000)\n",
    "            )\n",
    "            df_small.dropna(inplace=True)\n",
    "            df_small.drop_duplicates(inplace=True)\n",
    "            df = pd.concat([df, df_small], axis=0).reset_index(drop=True)\n",
    "        df[\"pos\"] = \"0_0\"\n",
    "        df.loc[\n",
    "            (df[\"latitude\"] >= Y - geo_split / 2)\n",
    "            & (df[\"latitude\"] <= Y + geo_split / 2)\n",
    "            & (df[\"longitude\"] >= X - geo_split / 2)\n",
    "            & (df[\"longitude\"] <= X + geo_split / 2),\n",
    "            \"pos\",\n",
    "        ] = (\n",
    "            str(Y - geo_split / 2) + \"_\" + str(X - geo_split / 2)\n",
    "        )\n",
    "        df[\"distance\"] = haversine_distance(df[\"latitude\"], df[\"longitude\"], Y, X)\n",
    "        df[\"latitude_disc\"] = Y - geo_split / 2\n",
    "        df[\"longitude_disc\"] = X - geo_split / 2\n",
    "        tmp = df[df[\"distance\"] <= radius]\n",
    "        if len(tmp) >= block_size + 1:\n",
    "            return (tmp,\n",
    "                start_time,\n",
    "                errors,\n",
    "            )\n",
    "        end_time = start_time\n",
    "        start_time = start_time - dt.timedelta(days=360)\n",
    "    print(\"No data\")\n",
    "    return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = dt.datetime.now()\n",
    "START_TIME = now - dt.timedelta(days=30)\n",
    "END_TIME = now\n",
    "GEO_SPLIT = 1\n",
    "RADIUS = 300\n",
    "BLOCK_SIZE = 64\n",
    "cnt = 0\n",
    "# for i in range(100_000):\n",
    "#     X = 16\n",
    "#     Y = 44\n",
    "#     MIN_LAT = Y - GEO_SPLIT / 2\n",
    "#     MAX_LAT = Y + GEO_SPLIT / 2\n",
    "#     MIN_LON = X - GEO_SPLIT / 2\n",
    "#     MAX_LON = X + GEO_SPLIT / 2\n",
    "#     try:\n",
    "#         df, start_time, errors = get_data(X, Y, START_TIME, END_TIME, MIN_LAT, MAX_LAT, MIN_LON, MAX_LON, GEO_SPLIT, RADIUS, BLOCK_SIZE)\n",
    "#         if df is not None:\n",
    "#             print(len(df))\n",
    "#             cnt += 1\n",
    "#             print(\"-\" * 20)\n",
    "#     except:\n",
    "#         print(X, Y, START_TIME, END_TIME, MIN_LAT, MAX_LAT, MIN_LON, MAX_LON, GEO_SPLIT, RADIUS, BLOCK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XD\n",
      "<Response [200]>\n",
      "2023-12-25 02:18:36.457885 3\n",
      "<Response [200]>\n",
      "XD\n",
      "<Response [200]>\n",
      "2022-12-30 02:18:36.457885 2024-01-01 18:14:49.054000 19488\n"
     ]
    }
   ],
   "source": [
    "X = -149\n",
    "Y = 60\n",
    "MIN_LAT = Y - GEO_SPLIT / 2\n",
    "MAX_LAT = Y + GEO_SPLIT / 2\n",
    "MIN_LON = X - GEO_SPLIT / 2\n",
    "MAX_LON = X + GEO_SPLIT / 2\n",
    "dates = get_data(X, Y, START_TIME, END_TIME, MIN_LAT, MAX_LAT, MIN_LON, MAX_LON, GEO_SPLIT, RADIUS, BLOCK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
