{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_paths(folder_path, recursive=False):\n",
    "    if recursive:\n",
    "        # Walk through all subdirectories\n",
    "        file_paths = [os.path.join(root, file) \n",
    "                      for root, _, files in os.walk(folder_path) \n",
    "                      for file in files if file.endswith('.csv')]\n",
    "    else:\n",
    "        # Get files in the root folder only\n",
    "        file_paths = [os.path.join(folder_path, file) \n",
    "                      for file in os.listdir(folder_path) \n",
    "                      if file.endswith('.csv')]\n",
    "    \n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>model</th>\n",
       "      <th>num_samples</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>avg_token_per_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nyt-comments</td>\n",
       "      <td>human</td>\n",
       "      <td>4223213</td>\n",
       "      <td>18713269</td>\n",
       "      <td>342590681</td>\n",
       "      <td>1418281599</td>\n",
       "      <td>367129360</td>\n",
       "      <td>86.931291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blogs</td>\n",
       "      <td>human</td>\n",
       "      <td>576731</td>\n",
       "      <td>8328325</td>\n",
       "      <td>150710195</td>\n",
       "      <td>557327652</td>\n",
       "      <td>164361476</td>\n",
       "      <td>284.988107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>raid</td>\n",
       "      <td>human</td>\n",
       "      <td>138244</td>\n",
       "      <td>1808791</td>\n",
       "      <td>46559309</td>\n",
       "      <td>215280947</td>\n",
       "      <td>95664352</td>\n",
       "      <td>691.996412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>natural-questions</td>\n",
       "      <td>human</td>\n",
       "      <td>231628</td>\n",
       "      <td>544546</td>\n",
       "      <td>12325923</td>\n",
       "      <td>52668992</td>\n",
       "      <td>14758408</td>\n",
       "      <td>63.715993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>writingprompts</td>\n",
       "      <td>human</td>\n",
       "      <td>303140</td>\n",
       "      <td>13802625</td>\n",
       "      <td>196423260</td>\n",
       "      <td>721935184</td>\n",
       "      <td>209317891</td>\n",
       "      <td>690.499080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>blogs</td>\n",
       "      <td>phi-4</td>\n",
       "      <td>28836</td>\n",
       "      <td>412861</td>\n",
       "      <td>6932437</td>\n",
       "      <td>27219979</td>\n",
       "      <td>7414709</td>\n",
       "      <td>257.133756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>blogs</td>\n",
       "      <td>gpt-4.1-nano-2025-04-14</td>\n",
       "      <td>28836</td>\n",
       "      <td>388053</td>\n",
       "      <td>7112835</td>\n",
       "      <td>27563134</td>\n",
       "      <td>7398632</td>\n",
       "      <td>256.576224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>blogs</td>\n",
       "      <td>Qwen2.5-7B-Instruct</td>\n",
       "      <td>28836</td>\n",
       "      <td>405020</td>\n",
       "      <td>6880299</td>\n",
       "      <td>26436051</td>\n",
       "      <td>7130797</td>\n",
       "      <td>247.288008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>blogs</td>\n",
       "      <td>Mistral-Nemo-Instruct-2407</td>\n",
       "      <td>28836</td>\n",
       "      <td>370692</td>\n",
       "      <td>6602922</td>\n",
       "      <td>24712562</td>\n",
       "      <td>6961997</td>\n",
       "      <td>241.434214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>blogs</td>\n",
       "      <td>Qwen2-72B-Instruct-AWQ</td>\n",
       "      <td>28835</td>\n",
       "      <td>590847</td>\n",
       "      <td>11402250</td>\n",
       "      <td>45952728</td>\n",
       "      <td>11875452</td>\n",
       "      <td>411.841581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>220 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  data                       model  num_samples  \\\n",
       "0         nyt-comments                       human      4223213   \n",
       "1                blogs                       human       576731   \n",
       "2                 raid                       human       138244   \n",
       "3    natural-questions                       human       231628   \n",
       "4       writingprompts                       human       303140   \n",
       "..                 ...                         ...          ...   \n",
       "215              blogs                       phi-4        28836   \n",
       "216              blogs     gpt-4.1-nano-2025-04-14        28836   \n",
       "217              blogs         Qwen2.5-7B-Instruct        28836   \n",
       "218              blogs  Mistral-Nemo-Instruct-2407        28836   \n",
       "219              blogs      Qwen2-72B-Instruct-AWQ        28835   \n",
       "\n",
       "     num_sentences  num_words   num_chars  num_tokens  avg_token_per_sample  \n",
       "0         18713269  342590681  1418281599   367129360             86.931291  \n",
       "1          8328325  150710195   557327652   164361476            284.988107  \n",
       "2          1808791   46559309   215280947    95664352            691.996412  \n",
       "3           544546   12325923    52668992    14758408             63.715993  \n",
       "4         13802625  196423260   721935184   209317891            690.499080  \n",
       "..             ...        ...         ...         ...                   ...  \n",
       "215         412861    6932437    27219979     7414709            257.133756  \n",
       "216         388053    7112835    27563134     7398632            256.576224  \n",
       "217         405020    6880299    26436051     7130797            247.288008  \n",
       "218         370692    6602922    24712562     6961997            241.434214  \n",
       "219         590847   11402250    45952728    11875452            411.841581  \n",
       "\n",
       "[220 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_main = pd.read_csv('../../data/stats/data_stats_master.csv')\n",
    "df_main[\"avg_token_per_sample\"] = df_main[\"num_tokens\"] / df_main[\"num_samples\"]\n",
    "df_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_HUMAN_PATH = \"../../data/stats/data_human\"\n",
    "DATA_AI_PATH = \"../../data/stats/data_ai\"\n",
    "DATASET_IDX_PATH = \"../../data/datasets/test3_idx.csv\"\n",
    "paths = get_csv_paths(DATA_HUMAN_PATH) + get_csv_paths(DATA_AI_PATH, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = dict({f\"{path.split(\"/\")[-1].split(\"_\")[0]}_{path.split(\"/\")[-1].split(\"_\")[1]}\": pd.read_csv(path) for path in paths})\n",
    "for k, v in stats.items():\n",
    "    stats[k] = v[v[\"num_tokens\"] <= 8192]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "def get_master_stats(stats: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    master_stats = {\n",
    "        \"data\": [],\n",
    "        \"model\": [],\n",
    "        \"num_samples\": [],\n",
    "        \"num_sentences\": [],\n",
    "        \"num_words\": [],\n",
    "        \"num_chars\": [],\n",
    "        \"num_tokens\": [],\n",
    "    }\n",
    "    for k, v in stats.items():\n",
    "        data, model = k.split(\"_\")\n",
    "        master_stats[\"data\"].append(data)\n",
    "        master_stats[\"model\"].append(model)\n",
    "        master_stats[\"num_samples\"].append(len(v))\n",
    "        for col in v.columns:\n",
    "            master_stats[col].append(v[col].sum())\n",
    "    df = pd.DataFrame(master_stats)\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_probs(df_main: pd.DataFrame, cols_c0: List[str]) -> pd.DataFrame:\n",
    "    df_main[\"avg_token_per_sample\"] = df_main[\"num_tokens\"] / df_main[\"num_samples\"]\n",
    "\n",
    "    for ds in df_main[\"data\"].unique():\n",
    "        df_main.loc[df_main[\"data\"].values == ds, \"prob\"] = (\n",
    "            1 / df_main.loc[df_main[\"data\"].values == ds, \"avg_token_per_sample\"].values\n",
    "        ) / (\n",
    "            1 / df_main.loc[df_main[\"data\"].values == ds, \"avg_token_per_sample\"]\n",
    "        ).sum()\n",
    "        mask_c0 = (df_main[\"data\"].values == ds) & (df_main[\"model\"].isin(cols_c0))\n",
    "        mask_c1 = (df_main[\"data\"].values == ds) & (~df_main[\"model\"].isin(cols_c0))\n",
    "\n",
    "        class0 = df_main[mask_c0]\n",
    "        class1 = df_main[mask_c1]\n",
    "\n",
    "        s1 = (class0[\"avg_token_per_sample\"] * class0[\"prob\"]).sum()\n",
    "        s2 = (class1[\"avg_token_per_sample\"] * class1[\"prob\"]).sum()\n",
    "        p1 = class0[\"prob\"].sum()\n",
    "        p2 = class1[\"prob\"].sum()\n",
    "\n",
    "        c1 = 1 / (s2 / s1 * p1 + p2)\n",
    "        c0 = c1 * s2 / s1\n",
    "\n",
    "        df_main.loc[mask_c0, \"prob\"] *= c0\n",
    "        df_main.loc[mask_c1, \"prob\"] *= c1\n",
    "\n",
    "    return df_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = get_master_stats(stats)\n",
    "df_main = calculate_probs(df_main, [\"human\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>model</th>\n",
       "      <th>num_samples</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>avg_token_per_sample</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>blogs</td>\n",
       "      <td>human</td>\n",
       "      <td>576632</td>\n",
       "      <td>8278264</td>\n",
       "      <td>149799988</td>\n",
       "      <td>553852318</td>\n",
       "      <td>163310829</td>\n",
       "      <td>283.214995</td>\n",
       "      <td>0.506773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>blogs</td>\n",
       "      <td>Ministral-8B-Instruct-2410</td>\n",
       "      <td>28423</td>\n",
       "      <td>327060</td>\n",
       "      <td>5780431</td>\n",
       "      <td>21880227</td>\n",
       "      <td>5967332</td>\n",
       "      <td>209.947296</td>\n",
       "      <td>0.032554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>blogs</td>\n",
       "      <td>Phi-3-small-128k-instruct</td>\n",
       "      <td>28556</td>\n",
       "      <td>486577</td>\n",
       "      <td>9732865</td>\n",
       "      <td>39712544</td>\n",
       "      <td>9995757</td>\n",
       "      <td>350.040517</td>\n",
       "      <td>0.019525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>blogs</td>\n",
       "      <td>Qwen2-7B-Instruct</td>\n",
       "      <td>28817</td>\n",
       "      <td>442256</td>\n",
       "      <td>9554373</td>\n",
       "      <td>40416854</td>\n",
       "      <td>9864740</td>\n",
       "      <td>342.323628</td>\n",
       "      <td>0.019965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>blogs</td>\n",
       "      <td>Qwen2.5-3B-Instruct</td>\n",
       "      <td>28691</td>\n",
       "      <td>369457</td>\n",
       "      <td>6297449</td>\n",
       "      <td>24656943</td>\n",
       "      <td>6610644</td>\n",
       "      <td>230.408281</td>\n",
       "      <td>0.029663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>blogs</td>\n",
       "      <td>Phi-3-medium-128k-instruct</td>\n",
       "      <td>28374</td>\n",
       "      <td>626548</td>\n",
       "      <td>12211407</td>\n",
       "      <td>49949200</td>\n",
       "      <td>12643982</td>\n",
       "      <td>445.618594</td>\n",
       "      <td>0.015337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>blogs</td>\n",
       "      <td>gpt-4.1-nano-2025-04-14</td>\n",
       "      <td>28831</td>\n",
       "      <td>387962</td>\n",
       "      <td>6981435</td>\n",
       "      <td>27314590</td>\n",
       "      <td>7234827</td>\n",
       "      <td>250.939163</td>\n",
       "      <td>0.027236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>blogs</td>\n",
       "      <td>phi-4</td>\n",
       "      <td>28807</td>\n",
       "      <td>400179</td>\n",
       "      <td>6657379</td>\n",
       "      <td>25892557</td>\n",
       "      <td>6965826</td>\n",
       "      <td>241.810185</td>\n",
       "      <td>0.028264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>blogs</td>\n",
       "      <td>Llama-3.2-3B-Instruct</td>\n",
       "      <td>28818</td>\n",
       "      <td>441771</td>\n",
       "      <td>10186381</td>\n",
       "      <td>41664330</td>\n",
       "      <td>10422019</td>\n",
       "      <td>361.649629</td>\n",
       "      <td>0.018898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>blogs</td>\n",
       "      <td>Qwen2.5-72B-Instruct-AWQ</td>\n",
       "      <td>28818</td>\n",
       "      <td>414282</td>\n",
       "      <td>7389139</td>\n",
       "      <td>27489484</td>\n",
       "      <td>7602582</td>\n",
       "      <td>263.813658</td>\n",
       "      <td>0.025907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>blogs</td>\n",
       "      <td>Falcon3-3B-Instruct</td>\n",
       "      <td>28715</td>\n",
       "      <td>464189</td>\n",
       "      <td>9837221</td>\n",
       "      <td>40723483</td>\n",
       "      <td>10286141</td>\n",
       "      <td>358.214905</td>\n",
       "      <td>0.019079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>blogs</td>\n",
       "      <td>Mistral-Nemo-Instruct-2407</td>\n",
       "      <td>28777</td>\n",
       "      <td>354388</td>\n",
       "      <td>5864416</td>\n",
       "      <td>22343814</td>\n",
       "      <td>6120481</td>\n",
       "      <td>212.686555</td>\n",
       "      <td>0.032134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>blogs</td>\n",
       "      <td>Qwen2-72B-Instruct-AWQ</td>\n",
       "      <td>28812</td>\n",
       "      <td>579105</td>\n",
       "      <td>11181238</td>\n",
       "      <td>44861179</td>\n",
       "      <td>11563870</td>\n",
       "      <td>401.356032</td>\n",
       "      <td>0.017029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>blogs</td>\n",
       "      <td>Meta-Llama-3.3-70B-Instruct-AWQ-INT4</td>\n",
       "      <td>28834</td>\n",
       "      <td>361778</td>\n",
       "      <td>8131651</td>\n",
       "      <td>32974977</td>\n",
       "      <td>8298305</td>\n",
       "      <td>287.795831</td>\n",
       "      <td>0.023748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>blogs</td>\n",
       "      <td>Qwen2.5-7B-Instruct</td>\n",
       "      <td>28823</td>\n",
       "      <td>384288</td>\n",
       "      <td>6723848</td>\n",
       "      <td>25982176</td>\n",
       "      <td>6952129</td>\n",
       "      <td>241.200742</td>\n",
       "      <td>0.028336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>blogs</td>\n",
       "      <td>Phi-3.5-mini-instruct</td>\n",
       "      <td>27762</td>\n",
       "      <td>856239</td>\n",
       "      <td>14898643</td>\n",
       "      <td>62235468</td>\n",
       "      <td>15846334</td>\n",
       "      <td>570.792234</td>\n",
       "      <td>0.011974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>blogs</td>\n",
       "      <td>Llama-3.1-8B-Instruct</td>\n",
       "      <td>28822</td>\n",
       "      <td>470806</td>\n",
       "      <td>10709997</td>\n",
       "      <td>43286691</td>\n",
       "      <td>10926605</td>\n",
       "      <td>379.106412</td>\n",
       "      <td>0.018028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>blogs</td>\n",
       "      <td>Phi-3-mini-128k-instruct</td>\n",
       "      <td>28061</td>\n",
       "      <td>744594</td>\n",
       "      <td>10951115</td>\n",
       "      <td>42325657</td>\n",
       "      <td>11504634</td>\n",
       "      <td>409.986601</td>\n",
       "      <td>0.016670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>blogs</td>\n",
       "      <td>Falcon3-7B-Instruct</td>\n",
       "      <td>28803</td>\n",
       "      <td>325605</td>\n",
       "      <td>6531027</td>\n",
       "      <td>26861591</td>\n",
       "      <td>6931598</td>\n",
       "      <td>240.655418</td>\n",
       "      <td>0.028400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>blogs</td>\n",
       "      <td>Phi-4-mini-instruct</td>\n",
       "      <td>28368</td>\n",
       "      <td>425222</td>\n",
       "      <td>7539898</td>\n",
       "      <td>28257306</td>\n",
       "      <td>7959100</td>\n",
       "      <td>280.566131</td>\n",
       "      <td>0.024360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>blogs</td>\n",
       "      <td>Meta-Llama-3.1-70B-Instruct-AWQ-INT4</td>\n",
       "      <td>28827</td>\n",
       "      <td>360259</td>\n",
       "      <td>7872634</td>\n",
       "      <td>31879700</td>\n",
       "      <td>8047970</td>\n",
       "      <td>279.181670</td>\n",
       "      <td>0.024481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>blogs</td>\n",
       "      <td>Qwen2.5-14B-Instruct</td>\n",
       "      <td>28831</td>\n",
       "      <td>330317</td>\n",
       "      <td>6032846</td>\n",
       "      <td>23853350</td>\n",
       "      <td>6227985</td>\n",
       "      <td>216.016961</td>\n",
       "      <td>0.031639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     data                                 model  num_samples  num_sentences  \\\n",
       "7   blogs                                 human       576632        8278264   \n",
       "10  blogs            Ministral-8B-Instruct-2410        28423         327060   \n",
       "11  blogs             Phi-3-small-128k-instruct        28556         486577   \n",
       "12  blogs                     Qwen2-7B-Instruct        28817         442256   \n",
       "13  blogs                   Qwen2.5-3B-Instruct        28691         369457   \n",
       "14  blogs            Phi-3-medium-128k-instruct        28374         626548   \n",
       "15  blogs               gpt-4.1-nano-2025-04-14        28831         387962   \n",
       "16  blogs                                 phi-4        28807         400179   \n",
       "17  blogs                 Llama-3.2-3B-Instruct        28818         441771   \n",
       "18  blogs              Qwen2.5-72B-Instruct-AWQ        28818         414282   \n",
       "19  blogs                   Falcon3-3B-Instruct        28715         464189   \n",
       "20  blogs            Mistral-Nemo-Instruct-2407        28777         354388   \n",
       "21  blogs                Qwen2-72B-Instruct-AWQ        28812         579105   \n",
       "22  blogs  Meta-Llama-3.3-70B-Instruct-AWQ-INT4        28834         361778   \n",
       "23  blogs                   Qwen2.5-7B-Instruct        28823         384288   \n",
       "24  blogs                 Phi-3.5-mini-instruct        27762         856239   \n",
       "25  blogs                 Llama-3.1-8B-Instruct        28822         470806   \n",
       "26  blogs              Phi-3-mini-128k-instruct        28061         744594   \n",
       "27  blogs                   Falcon3-7B-Instruct        28803         325605   \n",
       "28  blogs                   Phi-4-mini-instruct        28368         425222   \n",
       "29  blogs  Meta-Llama-3.1-70B-Instruct-AWQ-INT4        28827         360259   \n",
       "30  blogs                  Qwen2.5-14B-Instruct        28831         330317   \n",
       "\n",
       "    num_words  num_chars  num_tokens  avg_token_per_sample      prob  \n",
       "7   149799988  553852318   163310829            283.214995  0.506773  \n",
       "10    5780431   21880227     5967332            209.947296  0.032554  \n",
       "11    9732865   39712544     9995757            350.040517  0.019525  \n",
       "12    9554373   40416854     9864740            342.323628  0.019965  \n",
       "13    6297449   24656943     6610644            230.408281  0.029663  \n",
       "14   12211407   49949200    12643982            445.618594  0.015337  \n",
       "15    6981435   27314590     7234827            250.939163  0.027236  \n",
       "16    6657379   25892557     6965826            241.810185  0.028264  \n",
       "17   10186381   41664330    10422019            361.649629  0.018898  \n",
       "18    7389139   27489484     7602582            263.813658  0.025907  \n",
       "19    9837221   40723483    10286141            358.214905  0.019079  \n",
       "20    5864416   22343814     6120481            212.686555  0.032134  \n",
       "21   11181238   44861179    11563870            401.356032  0.017029  \n",
       "22    8131651   32974977     8298305            287.795831  0.023748  \n",
       "23    6723848   25982176     6952129            241.200742  0.028336  \n",
       "24   14898643   62235468    15846334            570.792234  0.011974  \n",
       "25   10709997   43286691    10926605            379.106412  0.018028  \n",
       "26   10951115   42325657    11504634            409.986601  0.016670  \n",
       "27    6531027   26861591     6931598            240.655418  0.028400  \n",
       "28    7539898   28257306     7959100            280.566131  0.024360  \n",
       "29    7872634   31879700     8047970            279.181670  0.024481  \n",
       "30    6032846   23853350     6227985            216.016961  0.031639  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_main[df_main[\"data\"] == \"blogs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 5e7\n",
    "total_tokens = 0\n",
    "total_sentences = 0\n",
    "total_samples = 0\n",
    "batch_size = 16\n",
    "cols_c0 = [\"human\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in df_main[\"data\"].unique():\n",
    "    df_main.loc[df_main[\"data\"].values == ds, \"prob\"] =  (1 / df_main.loc[df_main[\"data\"].values == ds, \"avg_token_per_sample\"].values) /  (1 / df_main.loc[df_main[\"data\"].values == ds, \"avg_token_per_sample\"]).sum()\n",
    "    mask_c0 = (df_main[\"data\"].values == ds) & (df_main[\"model\"].isin(cols_c0))\n",
    "    mask_c1 = (df_main[\"data\"].values == ds) & (~df_main[\"model\"].isin(cols_c0))\n",
    "\n",
    "    class0 = df_main[mask_c0]\n",
    "    class1 = df_main[mask_c1]\n",
    "\n",
    "    s1 = (class0[\"avg_token_per_sample\"] * class0[\"prob\"]).sum()\n",
    "    s2 = (class1[\"avg_token_per_sample\"] * class1[\"prob\"]).sum()\n",
    "    p1 = class0[\"prob\"].sum()\n",
    "    p2 = class1[\"prob\"].sum()\n",
    "\n",
    "    c1 = 1 / (s2 / s1 * p1 + p2)\n",
    "    c0 = c1 * s2 / s1\n",
    "\n",
    "    df_main.loc[mask_c0, \"prob\"] *= c0\n",
    "    df_main.loc[mask_c1, \"prob\"] *= c1\n",
    "\n",
    "weights = [\n",
    "    (df_main.loc[df_main[\"data\"] == ds, \"num_tokens\"] * df_main.loc[df_main[\"data\"] == ds, \"prob\"]).sum()\n",
    "    for ds in df_main[\"data\"].unique()\n",
    "]\n",
    "probs = np.array(weights) / np.sum(weights)\n",
    "\n",
    "# total_tokens = 0\n",
    "# total_sentences = 0\n",
    "# total_samples = 0\n",
    "# cnt = 0\n",
    "# while total_tokens < max_tokens:\n",
    "#     data = np.random.choice(df_main[\"data\"].unique(), p=probs)\n",
    "#     tmp = df_main[(df_main[\"data\"] == data)]\n",
    "#     model = np.random.choice(tmp[\"model\"], p=tmp[\"prob\"])\n",
    "\n",
    "#     stat = stats[f\"{data}_{model}\"]\n",
    "\n",
    "#     slct = stat.sample(n=batch_size)\n",
    "#     stat.drop(slct.index, inplace=True)\n",
    "\n",
    "#     total_tokens += slct.sum()[\"num_tokens\"]\n",
    "#     total_sentences += slct.sum()[\"num_sentences\"]\n",
    "#     total_samples += batch_size\n",
    "\n",
    "\n",
    "#     # save data, model, slct.index to csv\n",
    "#     slct[\"data\"] = data\n",
    "#     slct[\"model\"] = model\n",
    "#     slct.reset_index(inplace=True)\n",
    "#     # slct.drop(columns=[\"num_sentences\", \"num_words\", \"num_chars\", \"num_tokens\"], inplace=True)\n",
    "#     slct.to_csv(DATASET_IDX_PATH, mode='a', header=not os.path.exists(DATASET_IDX_PATH), index=False)\n",
    "\n",
    "#     cnt += 1\n",
    "#     if cnt % 1000 == 0:\n",
    "#         print(f\"total_tokens: {total_tokens}, total_sentences: {total_sentences}, total_samples: {total_samples}\")\n",
    "\n",
    "# print(\n",
    "#     f\"Final samples: {total_samples}, Final sentences: {total_sentences}, Final tokens: {total_tokens}\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>model</th>\n",
       "      <th>num_samples</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>avg_token_per_sample</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blogs</td>\n",
       "      <td>humanMinistral-8B-Instruct-2410Phi-3-small-128...</td>\n",
       "      <td>1178202</td>\n",
       "      <td>17831146</td>\n",
       "      <td>330865381</td>\n",
       "      <td>1278414439</td>\n",
       "      <td>351283690</td>\n",
       "      <td>6857.325439</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>essays</td>\n",
       "      <td>humanphi-4Ministral-8B-Instruct-2410gpt-4.1-na...</td>\n",
       "      <td>57578</td>\n",
       "      <td>2145711</td>\n",
       "      <td>33391899</td>\n",
       "      <td>119943300</td>\n",
       "      <td>32774134</td>\n",
       "      <td>12512.632963</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>natural-questions</td>\n",
       "      <td>humanQwen2-7B-InstructMeta-Llama-3.3-70B-Instr...</td>\n",
       "      <td>472560</td>\n",
       "      <td>1337769</td>\n",
       "      <td>28095003</td>\n",
       "      <td>120576958</td>\n",
       "      <td>33254831</td>\n",
       "      <td>1686.200278</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nyt-articles</td>\n",
       "      <td>humanFalcon3-7B-InstructPhi-4-mini-instructQwe...</td>\n",
       "      <td>347831</td>\n",
       "      <td>2150590</td>\n",
       "      <td>59458545</td>\n",
       "      <td>303014797</td>\n",
       "      <td>63217304</td>\n",
       "      <td>3999.040963</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nyt-comments</td>\n",
       "      <td>humanQwen2.5-14B-InstructPhi-3.5-mini-instruct...</td>\n",
       "      <td>8655021</td>\n",
       "      <td>34636096</td>\n",
       "      <td>674001938</td>\n",
       "      <td>2957694720</td>\n",
       "      <td>709465295</td>\n",
       "      <td>1709.175772</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>raid</td>\n",
       "      <td>humanPhi-3-small-128k-instructPhi-4-mini-instr...</td>\n",
       "      <td>862420</td>\n",
       "      <td>10457196</td>\n",
       "      <td>250151060</td>\n",
       "      <td>1144041635</td>\n",
       "      <td>309592286</td>\n",
       "      <td>7051.875860</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>reddit</td>\n",
       "      <td>humanPhi-3-mini-128k-instructMeta-Llama-3.3-70...</td>\n",
       "      <td>3398891</td>\n",
       "      <td>11007460</td>\n",
       "      <td>188317684</td>\n",
       "      <td>768644650</td>\n",
       "      <td>199528671</td>\n",
       "      <td>1328.587421</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tweets</td>\n",
       "      <td>humanFalcon3-7B-InstructFalcon3-3B-InstructLla...</td>\n",
       "      <td>3663591</td>\n",
       "      <td>7641000</td>\n",
       "      <td>85125304</td>\n",
       "      <td>328381727</td>\n",
       "      <td>95481119</td>\n",
       "      <td>580.272067</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>writingprompts</td>\n",
       "      <td>humanQwen2-72B-Instruct-AWQQwen2.5-7B-Instruct...</td>\n",
       "      <td>621132</td>\n",
       "      <td>23004906</td>\n",
       "      <td>374931823</td>\n",
       "      <td>1447750598</td>\n",
       "      <td>394793174</td>\n",
       "      <td>12940.854513</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>xsum</td>\n",
       "      <td>humanQwen2.5-14B-InstructPhi-3-small-128k-inst...</td>\n",
       "      <td>939348</td>\n",
       "      <td>12779150</td>\n",
       "      <td>337633028</td>\n",
       "      <td>1570106286</td>\n",
       "      <td>358037119</td>\n",
       "      <td>7897.974200</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                data                                              model  \\\n",
       "0              blogs  humanMinistral-8B-Instruct-2410Phi-3-small-128...   \n",
       "1             essays  humanphi-4Ministral-8B-Instruct-2410gpt-4.1-na...   \n",
       "2  natural-questions  humanQwen2-7B-InstructMeta-Llama-3.3-70B-Instr...   \n",
       "3       nyt-articles  humanFalcon3-7B-InstructPhi-4-mini-instructQwe...   \n",
       "4       nyt-comments  humanQwen2.5-14B-InstructPhi-3.5-mini-instruct...   \n",
       "5               raid  humanPhi-3-small-128k-instructPhi-4-mini-instr...   \n",
       "6             reddit  humanPhi-3-mini-128k-instructMeta-Llama-3.3-70...   \n",
       "7             tweets  humanFalcon3-7B-InstructFalcon3-3B-InstructLla...   \n",
       "8     writingprompts  humanQwen2-72B-Instruct-AWQQwen2.5-7B-Instruct...   \n",
       "9               xsum  humanQwen2.5-14B-InstructPhi-3-small-128k-inst...   \n",
       "\n",
       "   num_samples  num_sentences  num_words   num_chars  num_tokens  \\\n",
       "0      1178202       17831146  330865381  1278414439   351283690   \n",
       "1        57578        2145711   33391899   119943300    32774134   \n",
       "2       472560        1337769   28095003   120576958    33254831   \n",
       "3       347831        2150590   59458545   303014797    63217304   \n",
       "4      8655021       34636096  674001938  2957694720   709465295   \n",
       "5       862420       10457196  250151060  1144041635   309592286   \n",
       "6      3398891       11007460  188317684   768644650   199528671   \n",
       "7      3663591        7641000   85125304   328381727    95481119   \n",
       "8       621132       23004906  374931823  1447750598   394793174   \n",
       "9       939348       12779150  337633028  1570106286   358037119   \n",
       "\n",
       "   avg_token_per_sample  prob  \n",
       "0           6857.325439   1.0  \n",
       "1          12512.632963   1.0  \n",
       "2           1686.200278   1.0  \n",
       "3           3999.040963   1.0  \n",
       "4           1709.175772   1.0  \n",
       "5           7051.875860   1.0  \n",
       "6           1328.587421   1.0  \n",
       "7            580.272067   1.0  \n",
       "8          12940.854513   1.0  \n",
       "9           7897.974200   1.0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tmp = df_main.groupby(\"data\").sum().reset_index()\n",
    "df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>model</th>\n",
       "      <th>num_samples</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>avg_token_per_sample</th>\n",
       "      <th>prob</th>\n",
       "      <th>weight</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blogs</td>\n",
       "      <td>humanMinistral-8B-Instruct-2410Phi-3-small-128...</td>\n",
       "      <td>1178202</td>\n",
       "      <td>17831146</td>\n",
       "      <td>330865381</td>\n",
       "      <td>1278414439</td>\n",
       "      <td>351283690</td>\n",
       "      <td>6857.325439</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.758430e+08</td>\n",
       "      <td>0.366148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>essays</td>\n",
       "      <td>humanphi-4Ministral-8B-Instruct-2410gpt-4.1-na...</td>\n",
       "      <td>57578</td>\n",
       "      <td>2145711</td>\n",
       "      <td>33391899</td>\n",
       "      <td>119943300</td>\n",
       "      <td>32774134</td>\n",
       "      <td>12512.632963</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.194271e+05</td>\n",
       "      <td>0.001498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>natural-questions</td>\n",
       "      <td>humanQwen2-7B-InstructMeta-Llama-3.3-70B-Instr...</td>\n",
       "      <td>472560</td>\n",
       "      <td>1337769</td>\n",
       "      <td>28095003</td>\n",
       "      <td>120576958</td>\n",
       "      <td>33254831</td>\n",
       "      <td>1686.200278</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.310285e+07</td>\n",
       "      <td>0.068928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nyt-articles</td>\n",
       "      <td>humanFalcon3-7B-InstructPhi-4-mini-instructQwe...</td>\n",
       "      <td>347831</td>\n",
       "      <td>2150590</td>\n",
       "      <td>59458545</td>\n",
       "      <td>303014797</td>\n",
       "      <td>63217304</td>\n",
       "      <td>3999.040963</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.649805e+06</td>\n",
       "      <td>0.003435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nyt-comments</td>\n",
       "      <td>humanQwen2.5-14B-InstructPhi-3.5-mini-instruct...</td>\n",
       "      <td>8655021</td>\n",
       "      <td>34636096</td>\n",
       "      <td>674001938</td>\n",
       "      <td>2957694720</td>\n",
       "      <td>709465295</td>\n",
       "      <td>1709.175772</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.393383e+06</td>\n",
       "      <td>0.013313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>raid</td>\n",
       "      <td>humanPhi-3-small-128k-instructPhi-4-mini-instr...</td>\n",
       "      <td>862420</td>\n",
       "      <td>10457196</td>\n",
       "      <td>250151060</td>\n",
       "      <td>1144041635</td>\n",
       "      <td>309592286</td>\n",
       "      <td>7051.875860</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.957258e+07</td>\n",
       "      <td>0.207334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>reddit</td>\n",
       "      <td>humanPhi-3-mini-128k-instructMeta-Llama-3.3-70...</td>\n",
       "      <td>3398891</td>\n",
       "      <td>11007460</td>\n",
       "      <td>188317684</td>\n",
       "      <td>768644650</td>\n",
       "      <td>199528671</td>\n",
       "      <td>1328.587421</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.530097e+06</td>\n",
       "      <td>0.015679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tweets</td>\n",
       "      <td>humanFalcon3-7B-InstructFalcon3-3B-InstructLla...</td>\n",
       "      <td>3663591</td>\n",
       "      <td>7641000</td>\n",
       "      <td>85125304</td>\n",
       "      <td>328381727</td>\n",
       "      <td>95481119</td>\n",
       "      <td>580.272067</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.687302e+07</td>\n",
       "      <td>0.180891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>writingprompts</td>\n",
       "      <td>humanQwen2-72B-Instruct-AWQQwen2.5-7B-Instruct...</td>\n",
       "      <td>621132</td>\n",
       "      <td>23004906</td>\n",
       "      <td>374931823</td>\n",
       "      <td>1447750598</td>\n",
       "      <td>394793174</td>\n",
       "      <td>12940.854513</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.074211e+07</td>\n",
       "      <td>0.043190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>xsum</td>\n",
       "      <td>humanQwen2.5-14B-InstructPhi-3-small-128k-inst...</td>\n",
       "      <td>939348</td>\n",
       "      <td>12779150</td>\n",
       "      <td>337633028</td>\n",
       "      <td>1570106286</td>\n",
       "      <td>358037119</td>\n",
       "      <td>7897.974200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.782538e+07</td>\n",
       "      <td>0.099584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                data                                              model  \\\n",
       "0              blogs  humanMinistral-8B-Instruct-2410Phi-3-small-128...   \n",
       "1             essays  humanphi-4Ministral-8B-Instruct-2410gpt-4.1-na...   \n",
       "2  natural-questions  humanQwen2-7B-InstructMeta-Llama-3.3-70B-Instr...   \n",
       "3       nyt-articles  humanFalcon3-7B-InstructPhi-4-mini-instructQwe...   \n",
       "4       nyt-comments  humanQwen2.5-14B-InstructPhi-3.5-mini-instruct...   \n",
       "5               raid  humanPhi-3-small-128k-instructPhi-4-mini-instr...   \n",
       "6             reddit  humanPhi-3-mini-128k-instructMeta-Llama-3.3-70...   \n",
       "7             tweets  humanFalcon3-7B-InstructFalcon3-3B-InstructLla...   \n",
       "8     writingprompts  humanQwen2-72B-Instruct-AWQQwen2.5-7B-Instruct...   \n",
       "9               xsum  humanQwen2.5-14B-InstructPhi-3-small-128k-inst...   \n",
       "\n",
       "   num_samples  num_sentences  num_words   num_chars  num_tokens  \\\n",
       "0      1178202       17831146  330865381  1278414439   351283690   \n",
       "1        57578        2145711   33391899   119943300    32774134   \n",
       "2       472560        1337769   28095003   120576958    33254831   \n",
       "3       347831        2150590   59458545   303014797    63217304   \n",
       "4      8655021       34636096  674001938  2957694720   709465295   \n",
       "5       862420       10457196  250151060  1144041635   309592286   \n",
       "6      3398891       11007460  188317684   768644650   199528671   \n",
       "7      3663591        7641000   85125304   328381727    95481119   \n",
       "8       621132       23004906  374931823  1447750598   394793174   \n",
       "9       939348       12779150  337633028  1570106286   358037119   \n",
       "\n",
       "   avg_token_per_sample  prob        weight  probability  \n",
       "0           6857.325439   1.0  1.758430e+08     0.366148  \n",
       "1          12512.632963   1.0  7.194271e+05     0.001498  \n",
       "2           1686.200278   1.0  3.310285e+07     0.068928  \n",
       "3           3999.040963   1.0  1.649805e+06     0.003435  \n",
       "4           1709.175772   1.0  6.393383e+06     0.013313  \n",
       "5           7051.875860   1.0  9.957258e+07     0.207334  \n",
       "6           1328.587421   1.0  7.530097e+06     0.015679  \n",
       "7            580.272067   1.0  8.687302e+07     0.180891  \n",
       "8          12940.854513   1.0  2.074211e+07     0.043190  \n",
       "9           7897.974200   1.0  4.782538e+07     0.099584  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tmp[\"weight\"] = weights\n",
    "df_tmp[\"probability\"] = probs\n",
    "df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>weight</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blogs</td>\n",
       "      <td>351283690</td>\n",
       "      <td>1.758430e+08</td>\n",
       "      <td>0.366148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>essays</td>\n",
       "      <td>32774134</td>\n",
       "      <td>7.194271e+05</td>\n",
       "      <td>0.001498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>natural-questions</td>\n",
       "      <td>33254831</td>\n",
       "      <td>3.310285e+07</td>\n",
       "      <td>0.068928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nyt-articles</td>\n",
       "      <td>63217304</td>\n",
       "      <td>1.649805e+06</td>\n",
       "      <td>0.003435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nyt-comments</td>\n",
       "      <td>709465295</td>\n",
       "      <td>6.393383e+06</td>\n",
       "      <td>0.013313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>raid</td>\n",
       "      <td>309592286</td>\n",
       "      <td>9.957258e+07</td>\n",
       "      <td>0.207334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>reddit</td>\n",
       "      <td>199528671</td>\n",
       "      <td>7.530097e+06</td>\n",
       "      <td>0.015679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tweets</td>\n",
       "      <td>95481119</td>\n",
       "      <td>8.687302e+07</td>\n",
       "      <td>0.180891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>writingprompts</td>\n",
       "      <td>394793174</td>\n",
       "      <td>2.074211e+07</td>\n",
       "      <td>0.043190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>xsum</td>\n",
       "      <td>358037119</td>\n",
       "      <td>4.782538e+07</td>\n",
       "      <td>0.099584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                data  num_tokens        weight  probability\n",
       "0              blogs   351283690  1.758430e+08     0.366148\n",
       "1             essays    32774134  7.194271e+05     0.001498\n",
       "2  natural-questions    33254831  3.310285e+07     0.068928\n",
       "3       nyt-articles    63217304  1.649805e+06     0.003435\n",
       "4       nyt-comments   709465295  6.393383e+06     0.013313\n",
       "5               raid   309592286  9.957258e+07     0.207334\n",
       "6             reddit   199528671  7.530097e+06     0.015679\n",
       "7             tweets    95481119  8.687302e+07     0.180891\n",
       "8     writingprompts   394793174  2.074211e+07     0.043190\n",
       "9               xsum   358037119  4.782538e+07     0.099584"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tmp[[\"data\", \"num_tokens\", \"weight\", \"probability\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>data</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22550</td>\n",
       "      <td>34</td>\n",
       "      <td>20</td>\n",
       "      <td>2522</td>\n",
       "      <td>636</td>\n",
       "      <td>blogs</td>\n",
       "      <td>Llama-3.1-8B-Instruct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9473</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "      <td>1787</td>\n",
       "      <td>405</td>\n",
       "      <td>blogs</td>\n",
       "      <td>Llama-3.1-8B-Instruct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20600</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>1480</td>\n",
       "      <td>323</td>\n",
       "      <td>blogs</td>\n",
       "      <td>Llama-3.1-8B-Instruct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23943</td>\n",
       "      <td>6</td>\n",
       "      <td>78</td>\n",
       "      <td>1058</td>\n",
       "      <td>244</td>\n",
       "      <td>blogs</td>\n",
       "      <td>Llama-3.1-8B-Instruct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20857</td>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "      <td>1790</td>\n",
       "      <td>478</td>\n",
       "      <td>blogs</td>\n",
       "      <td>Llama-3.1-8B-Instruct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  num_sentences  num_words  num_chars  num_tokens   data  \\\n",
       "0  22550             34         20       2522         636  blogs   \n",
       "1   9473             15         26       1787         405  blogs   \n",
       "2  20600             11         26       1480         323  blogs   \n",
       "3  23943              6         78       1058         244  blogs   \n",
       "4  20857             22         24       1790         478  blogs   \n",
       "\n",
       "                   model  \n",
       "0  Llama-3.1-8B-Instruct  \n",
       "1  Llama-3.1-8B-Instruct  \n",
       "2  Llama-3.1-8B-Instruct  \n",
       "3  Llama-3.1-8B-Instruct  \n",
       "4  Llama-3.1-8B-Instruct  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(DATASET_IDX_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"is_human\"] = np.where(df[\"model\"].isin(cols_c0), \"human\", \"ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>data</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_human</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ai</th>\n",
       "      <td>989425257</td>\n",
       "      <td>1245943</td>\n",
       "      <td>1864094</td>\n",
       "      <td>91922494</td>\n",
       "      <td>24886382</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>Llama-3.1-8B-InstructLlama-3.1-8B-InstructLlam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human</th>\n",
       "      <td>24154483641</td>\n",
       "      <td>1286201</td>\n",
       "      <td>1777485</td>\n",
       "      <td>85321878</td>\n",
       "      <td>25115887</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>humanhumanhumanhumanhumanhumanhumanhumanhumanh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                index  num_sentences  num_words  num_chars  num_tokens  \\\n",
       "is_human                                                                 \n",
       "ai          989425257        1245943    1864094   91922494    24886382   \n",
       "human     24154483641        1286201    1777485   85321878    25115887   \n",
       "\n",
       "                                                       data  \\\n",
       "is_human                                                      \n",
       "ai        blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "human     blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "\n",
       "                                                      model  \n",
       "is_human                                                     \n",
       "ai        Llama-3.1-8B-InstructLlama-3.1-8B-InstructLlam...  \n",
       "human     humanhumanhumanhumanhumanhumanhumanhumanhumanh...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"is_human\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>data</th>\n",
       "      <th>is_human</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Falcon3-3B-Instruct</th>\n",
       "      <td>40449566</td>\n",
       "      <td>56955</td>\n",
       "      <td>43858</td>\n",
       "      <td>4635235</td>\n",
       "      <td>1226797</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Falcon3-7B-Instruct</th>\n",
       "      <td>68720793</td>\n",
       "      <td>66238</td>\n",
       "      <td>89868</td>\n",
       "      <td>4854747</td>\n",
       "      <td>1283953</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama-3.1-8B-Instruct</th>\n",
       "      <td>41411787</td>\n",
       "      <td>48073</td>\n",
       "      <td>89742</td>\n",
       "      <td>4495386</td>\n",
       "      <td>1152977</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama-3.2-3B-Instruct</th>\n",
       "      <td>43652666</td>\n",
       "      <td>46631</td>\n",
       "      <td>70223</td>\n",
       "      <td>4399048</td>\n",
       "      <td>1108830</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Meta-Llama-3.1-70B-Instruct-AWQ-INT4</th>\n",
       "      <td>63983411</td>\n",
       "      <td>61399</td>\n",
       "      <td>104453</td>\n",
       "      <td>5374332</td>\n",
       "      <td>1370300</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Meta-Llama-3.3-70B-Instruct-AWQ-INT4</th>\n",
       "      <td>59456762</td>\n",
       "      <td>53363</td>\n",
       "      <td>94822</td>\n",
       "      <td>4889710</td>\n",
       "      <td>1242738</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ministral-8B-Instruct-2410</th>\n",
       "      <td>46685710</td>\n",
       "      <td>84620</td>\n",
       "      <td>91459</td>\n",
       "      <td>4686317</td>\n",
       "      <td>1337393</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mistral-Nemo-Instruct-2407</th>\n",
       "      <td>77235804</td>\n",
       "      <td>72022</td>\n",
       "      <td>141751</td>\n",
       "      <td>4969484</td>\n",
       "      <td>1406950</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phi-3-medium-128k-instruct</th>\n",
       "      <td>29849886</td>\n",
       "      <td>65417</td>\n",
       "      <td>128841</td>\n",
       "      <td>5067603</td>\n",
       "      <td>1349274</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phi-3-mini-128k-instruct</th>\n",
       "      <td>25448181</td>\n",
       "      <td>90611</td>\n",
       "      <td>214916</td>\n",
       "      <td>5202073</td>\n",
       "      <td>1401653</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phi-3-small-128k-instruct</th>\n",
       "      <td>30501319</td>\n",
       "      <td>50908</td>\n",
       "      <td>49944</td>\n",
       "      <td>4429728</td>\n",
       "      <td>1152225</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phi-3.5-mini-instruct</th>\n",
       "      <td>16121163</td>\n",
       "      <td>49014</td>\n",
       "      <td>146001</td>\n",
       "      <td>3690680</td>\n",
       "      <td>1128726</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phi-4-mini-instruct</th>\n",
       "      <td>33663988</td>\n",
       "      <td>71849</td>\n",
       "      <td>152293</td>\n",
       "      <td>4014292</td>\n",
       "      <td>1338002</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen2-72B-Instruct-AWQ</th>\n",
       "      <td>38405990</td>\n",
       "      <td>56129</td>\n",
       "      <td>42181</td>\n",
       "      <td>4356626</td>\n",
       "      <td>1130762</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen2-7B-Instruct</th>\n",
       "      <td>43453256</td>\n",
       "      <td>51986</td>\n",
       "      <td>58331</td>\n",
       "      <td>4791534</td>\n",
       "      <td>1174414</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen2.5-14B-Instruct</th>\n",
       "      <td>77507021</td>\n",
       "      <td>64178</td>\n",
       "      <td>78686</td>\n",
       "      <td>4662054</td>\n",
       "      <td>1232185</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen2.5-3B-Instruct</th>\n",
       "      <td>51593904</td>\n",
       "      <td>58719</td>\n",
       "      <td>71521</td>\n",
       "      <td>4026677</td>\n",
       "      <td>1119966</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen2.5-72B-Instruct-AWQ</th>\n",
       "      <td>63937051</td>\n",
       "      <td>64633</td>\n",
       "      <td>67706</td>\n",
       "      <td>4534268</td>\n",
       "      <td>1267075</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen2.5-7B-Instruct</th>\n",
       "      <td>72293626</td>\n",
       "      <td>67418</td>\n",
       "      <td>65840</td>\n",
       "      <td>4550287</td>\n",
       "      <td>1266062</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human</th>\n",
       "      <td>24154483641</td>\n",
       "      <td>1286201</td>\n",
       "      <td>1777485</td>\n",
       "      <td>85321878</td>\n",
       "      <td>25115887</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>humanhumanhumanhumanhumanhumanhumanhumanhumanh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phi-4</th>\n",
       "      <td>65053373</td>\n",
       "      <td>65780</td>\n",
       "      <td>61658</td>\n",
       "      <td>4292413</td>\n",
       "      <td>1196100</td>\n",
       "      <td>blogsblogsblogsblogsblogsblogsblogsblogsblogsb...</td>\n",
       "      <td>aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            index  num_sentences  num_words  \\\n",
       "model                                                                         \n",
       "Falcon3-3B-Instruct                      40449566          56955      43858   \n",
       "Falcon3-7B-Instruct                      68720793          66238      89868   \n",
       "Llama-3.1-8B-Instruct                    41411787          48073      89742   \n",
       "Llama-3.2-3B-Instruct                    43652666          46631      70223   \n",
       "Meta-Llama-3.1-70B-Instruct-AWQ-INT4     63983411          61399     104453   \n",
       "Meta-Llama-3.3-70B-Instruct-AWQ-INT4     59456762          53363      94822   \n",
       "Ministral-8B-Instruct-2410               46685710          84620      91459   \n",
       "Mistral-Nemo-Instruct-2407               77235804          72022     141751   \n",
       "Phi-3-medium-128k-instruct               29849886          65417     128841   \n",
       "Phi-3-mini-128k-instruct                 25448181          90611     214916   \n",
       "Phi-3-small-128k-instruct                30501319          50908      49944   \n",
       "Phi-3.5-mini-instruct                    16121163          49014     146001   \n",
       "Phi-4-mini-instruct                      33663988          71849     152293   \n",
       "Qwen2-72B-Instruct-AWQ                   38405990          56129      42181   \n",
       "Qwen2-7B-Instruct                        43453256          51986      58331   \n",
       "Qwen2.5-14B-Instruct                     77507021          64178      78686   \n",
       "Qwen2.5-3B-Instruct                      51593904          58719      71521   \n",
       "Qwen2.5-72B-Instruct-AWQ                 63937051          64633      67706   \n",
       "Qwen2.5-7B-Instruct                      72293626          67418      65840   \n",
       "human                                 24154483641        1286201    1777485   \n",
       "phi-4                                    65053373          65780      61658   \n",
       "\n",
       "                                      num_chars  num_tokens  \\\n",
       "model                                                         \n",
       "Falcon3-3B-Instruct                     4635235     1226797   \n",
       "Falcon3-7B-Instruct                     4854747     1283953   \n",
       "Llama-3.1-8B-Instruct                   4495386     1152977   \n",
       "Llama-3.2-3B-Instruct                   4399048     1108830   \n",
       "Meta-Llama-3.1-70B-Instruct-AWQ-INT4    5374332     1370300   \n",
       "Meta-Llama-3.3-70B-Instruct-AWQ-INT4    4889710     1242738   \n",
       "Ministral-8B-Instruct-2410              4686317     1337393   \n",
       "Mistral-Nemo-Instruct-2407              4969484     1406950   \n",
       "Phi-3-medium-128k-instruct              5067603     1349274   \n",
       "Phi-3-mini-128k-instruct                5202073     1401653   \n",
       "Phi-3-small-128k-instruct               4429728     1152225   \n",
       "Phi-3.5-mini-instruct                   3690680     1128726   \n",
       "Phi-4-mini-instruct                     4014292     1338002   \n",
       "Qwen2-72B-Instruct-AWQ                  4356626     1130762   \n",
       "Qwen2-7B-Instruct                       4791534     1174414   \n",
       "Qwen2.5-14B-Instruct                    4662054     1232185   \n",
       "Qwen2.5-3B-Instruct                     4026677     1119966   \n",
       "Qwen2.5-72B-Instruct-AWQ                4534268     1267075   \n",
       "Qwen2.5-7B-Instruct                     4550287     1266062   \n",
       "human                                  85321878    25115887   \n",
       "phi-4                                   4292413     1196100   \n",
       "\n",
       "                                                                                   data  \\\n",
       "model                                                                                     \n",
       "Falcon3-3B-Instruct                   blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Falcon3-7B-Instruct                   blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Llama-3.1-8B-Instruct                 blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Llama-3.2-3B-Instruct                 blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Meta-Llama-3.1-70B-Instruct-AWQ-INT4  blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Meta-Llama-3.3-70B-Instruct-AWQ-INT4  blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Ministral-8B-Instruct-2410            blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Mistral-Nemo-Instruct-2407            blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Phi-3-medium-128k-instruct            blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Phi-3-mini-128k-instruct              blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Phi-3-small-128k-instruct             blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Phi-3.5-mini-instruct                 blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Phi-4-mini-instruct                   blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Qwen2-72B-Instruct-AWQ                blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Qwen2-7B-Instruct                     blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Qwen2.5-14B-Instruct                  blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Qwen2.5-3B-Instruct                   blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Qwen2.5-72B-Instruct-AWQ              blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "Qwen2.5-7B-Instruct                   blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "human                                 blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "phi-4                                 blogsblogsblogsblogsblogsblogsblogsblogsblogsb...   \n",
       "\n",
       "                                                                               is_human  \n",
       "model                                                                                    \n",
       "Falcon3-3B-Instruct                   aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Falcon3-7B-Instruct                   aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Llama-3.1-8B-Instruct                 aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Llama-3.2-3B-Instruct                 aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Meta-Llama-3.1-70B-Instruct-AWQ-INT4  aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Meta-Llama-3.3-70B-Instruct-AWQ-INT4  aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Ministral-8B-Instruct-2410            aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Mistral-Nemo-Instruct-2407            aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Phi-3-medium-128k-instruct            aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Phi-3-mini-128k-instruct              aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Phi-3-small-128k-instruct             aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Phi-3.5-mini-instruct                 aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Phi-4-mini-instruct                   aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Qwen2-72B-Instruct-AWQ                aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Qwen2-7B-Instruct                     aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Qwen2.5-14B-Instruct                  aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Qwen2.5-3B-Instruct                   aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Qwen2.5-72B-Instruct-AWQ              aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "Qwen2.5-7B-Instruct                   aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  \n",
       "human                                 humanhumanhumanhumanhumanhumanhumanhumanhumanh...  \n",
       "phi-4                                 aiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiaiai...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"model\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
