{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Baseline modele trenowane na mastert test set - (4 modele x 4 datasety = 16 ewaluacji) \n",
    "2. 20 Llmow finetunowanych na master_large (?) - (20 modelow x 1 dataset = 20 ewaluacji)\n",
    "3. 20 llmow finetunowanych do detekcji samego siebie - (20 modelow x 1 datatset = 20 ewaluacji), tutaj agregacja potrzebna\n",
    "4. 20 llmow finetunowanych do deteckji rodziny (20 modelow x 1 dataset = 20 ewaluacji), agregacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Tuple, List, Dict\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, f1_score,\n",
    "                             precision_score, recall_score, roc_auc_score)\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], labels: List[int]) -> None:\n",
    "        \"\"\"\n",
    "        texts: list of texts.\n",
    "        labels: list of labels for all samples.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Union[str, int]]:\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return {\"text\": text, \"label\": label}\n",
    "\n",
    "\n",
    "def get_csv_paths(folder_path: str, recursive: bool = False) -> List[str]:\n",
    "    if recursive:\n",
    "        # Walk through all subdirectories\n",
    "        file_paths = [\n",
    "            os.path.join(root, file)\n",
    "            for root, _, files in os.walk(folder_path)\n",
    "            for file in files\n",
    "            if file.endswith(\".csv\")\n",
    "        ]\n",
    "    else:\n",
    "        # Get files in the root folder only\n",
    "        file_paths = [\n",
    "            os.path.join(folder_path, file)\n",
    "            for file in os.listdir(folder_path)\n",
    "            if file.endswith(\".csv\")\n",
    "        ]\n",
    "\n",
    "    return file_paths\n",
    "\n",
    "\n",
    "def collate_fn(\n",
    "    batch: List[Dict[str, torch.tensor]], tokenizer: AutoTokenizer\n",
    ") -> Dict[str, torch.tensor]:\n",
    "    texts = [item[\"text\"] for item in batch]\n",
    "    labels = [item[\"label\"] for item in batch]\n",
    "    encodings = tokenizer(\n",
    "        texts, truncation=True, padding=\"longest\", return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    labels_padded = [\n",
    "        torch.where(t == 0, torch.tensor(-100), torch.tensor(label))\n",
    "        for t, label in zip(encodings[\"attention_mask\"], labels)\n",
    "    ]\n",
    "    labels_padded = torch.cat(labels_padded)\n",
    "    encodings[\"labels\"] = labels_padded\n",
    "\n",
    "    return encodings\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model: torch.nn.Module, dataloader: DataLoader, device: str, type: str\n",
    ") -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    total_loss = 0.0\n",
    "    loss_fn = BCEWithLogitsLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            if type == \"baseline\":\n",
    "                outputs = model(input_ids)\n",
    "            elif type == \"finetune\":\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "            else:\n",
    "                raise Exception(\n",
    "                    \"Wrong training type, should be 'baseline' or 'finetune'.\"\n",
    "                )\n",
    "\n",
    "            mask = labels.view(-1) != -100\n",
    "            labels = labels.view(-1)[mask].float()\n",
    "            outputs = outputs.view(-1)[mask]\n",
    "\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            logits = torch.sigmoid(outputs).squeeze().cpu().numpy()\n",
    "            labels = labels.squeeze().cpu().numpy()\n",
    "\n",
    "            preds.extend(logits)\n",
    "            targets.extend(labels)\n",
    "\n",
    "    bin_preds = [1 if p >= 0.5 else 0 for p in preds]\n",
    "\n",
    "    metrics = {\n",
    "        \"loss\": total_loss / len(dataloader),\n",
    "        \"accuracy\": accuracy_score(targets, bin_preds),\n",
    "        \"balanced_accuracy\": balanced_accuracy_score(targets, bin_preds),\n",
    "        \"precision\": precision_score(targets, bin_preds),\n",
    "        \"recall\": recall_score(targets, bin_preds),\n",
    "        \"f1\": f1_score(targets, bin_preds),\n",
    "        \"auc\": roc_auc_score(targets, preds),\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "class FineTuneClassifier(nn.Module):\n",
    "    def __init__(self, base_model_path: str, num_labels: int) -> None:\n",
    "        super(FineTuneClassifier, self).__init__()\n",
    "        self.base_model = AutoModel.from_pretrained(base_model_path)\n",
    "\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.classifier = nn.Linear(self.base_model.config.hidden_size * 2, num_labels)\n",
    "\n",
    "    @classmethod\n",
    "    def from_classifier_head(\n",
    "        cls, base_model_path: str, path: str, num_labels: int\n",
    "    ) -> nn.Module:\n",
    "        model = cls(base_model_path, num_labels)\n",
    "        model.classifier.load_state_dict(torch.load(path))\n",
    "        return model\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids: torch.tensor, attention_mask: torch.tensor\n",
    "    ) -> torch.tensor:\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        B, T, C = outputs.logits.shape\n",
    "\n",
    "        all_tokens_hidden = outputs.logits  # (B, T, C)\n",
    "        last_token_hidden = outputs.logits[:, -1, :]  # (B, C)\n",
    "        last_token_hidden = last_token_hidden.unsqueeze(1).expand(B, T, C)\n",
    "\n",
    "        combined_representation = torch.cat(\n",
    "            (all_tokens_hidden, last_token_hidden), dim=-1\n",
    "        )\n",
    "        logits = self.classifier(combined_representation)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class BaselineClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        num_layers: int,\n",
    "        nhead: int,\n",
    "        max_seq_length: int,\n",
    "        vocab_size: int,\n",
    "        pad_token_id: int,\n",
    "        num_labels: int,\n",
    "    ) -> None:\n",
    "        super(BaselineClassifier, self).__init__()\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.token_embedding = nn.Embedding(\n",
    "            vocab_size, d_model, padding_idx=pad_token_id\n",
    "        )\n",
    "        self.pos_embedding = nn.Embedding(max_seq_length, d_model)\n",
    "        decoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(decoder_layer, num_layers=num_layers)\n",
    "        self.classifier = nn.Linear(d_model * 2, num_labels)\n",
    "\n",
    "    def forward(self, token_ids: torch.tensor) -> torch.tensor:\n",
    "        batch_size, seq_len = token_ids.shape\n",
    "\n",
    "        token_emb = self.token_embedding(token_ids)\n",
    "        pos_ids = torch.arange(seq_len, device=token_ids.device).unsqueeze(0)\n",
    "        pos_emb = self.pos_embedding(pos_ids)\n",
    "        embeddings = token_emb + pos_emb\n",
    "\n",
    "        causal_mask = torch.triu(\n",
    "            torch.ones(seq_len, seq_len, device=token_ids.device, dtype=torch.bool),\n",
    "            diagonal=1,\n",
    "        )\n",
    "\n",
    "        pad_mask = token_ids.eq(self.pad_token_id)  # shape: (batch_size, seq_len)\n",
    "\n",
    "        output = self.transformer(\n",
    "            embeddings, mask=causal_mask, src_key_padding_mask=pad_mask\n",
    "        )\n",
    "\n",
    "        B, T, C = output.shape\n",
    "        all_tokens_hidden = output  # (B, T, C)\n",
    "        last_token_hidden = output[:, -1, :]  # (B, C)\n",
    "        last_token_hidden = last_token_hidden.unsqueeze(1).expand(B, T, C)\n",
    "\n",
    "        combined_representation = torch.cat(\n",
    "            (all_tokens_hidden, last_token_hidden), dim=-1\n",
    "        )\n",
    "        logits = self.classifier(combined_representation)\n",
    "        return logits\n",
    "BASELINE_MODELS: Dict[str, Dict[str, int]] = {\n",
    "    \"mini\": {\n",
    "        \"d_model\": 512,\n",
    "        \"num_layers\": 6,\n",
    "        \"num_heads\": 8,\n",
    "        \"max_len\": 512,\n",
    "    },\n",
    "    \"small\": {\n",
    "        \"d_model\": 768,\n",
    "        \"num_layers\": 12,\n",
    "        \"num_heads\": 12,\n",
    "        \"max_len\": 512,\n",
    "    },\n",
    "    \"medium\": {\n",
    "        \"d_model\": 1024,\n",
    "        \"num_layers\": 24,\n",
    "        \"num_heads\": 16,\n",
    "        \"max_len\": 512,\n",
    "    },\n",
    "    \"large\": {\n",
    "        \"d_model\": 1536,\n",
    "        \"num_layers\": 36,\n",
    "        \"num_heads\": 24,\n",
    "        \"max_len\": 512,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINTS_PATH: str = \"../../../checkpoints/\"\n",
    "DATASETS_PATH: str = \"../../../data/datasets/\"\n",
    "BATCH_SIZE = 32\n",
    "baseline_checkpoints = get_csv_paths(CHECKPOINTS_PATH + \"baseline/\")\n",
    "finetune_checkpoints = get_csv_paths(CHECKPOINTS_PATH + \"finetune/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(DATASETS_PATH + \"master_testset/test.csv\")\n",
    "test_dataset = TextDataset(df_test[\"text\"].tolist(), df_test[\"labels\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\"model_name\": [], \"train_ds_name\": [], \"training_type\": [], \"loss\": [], \"accuracy\": [], \"balanced_accuracy\": [], \"precision\": [], \"recall\": [], \"f1\": [], \"auc\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in baseline_checkpoints:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "    tokenizer.pad_token = \"<|finetune_right_pad_id|>\"\n",
    "    test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda batch: collate_fn(batch, tokenizer),\n",
    "    )\n",
    "\n",
    "    model_size = path.split(\"_\")[1]\n",
    "    train_ds_name = path.split(\"_\")[2]\n",
    "    model_config = BASELINE_MODELS[model_size]\n",
    "    model = BaselineClassifier(model_config, num_labels=1)\n",
    "    model.load_state_dict(torch.load(path, weights_only=True))\n",
    "    metrics = evaluate(model, test_loader, \"cuda\", \"baseline\")\n",
    "\n",
    "    # save metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
