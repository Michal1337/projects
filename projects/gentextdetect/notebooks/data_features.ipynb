{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/majkel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/majkel/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/majkel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import statistics\n",
    "from collections import Counter\n",
    "from typing import List, Union\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import textstat\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from scipy.stats import entropy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Load SpaCy model for syntactic features\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_metric(string):\n",
    "    string_list = string.split()\n",
    "    counts = np.unique(string_list, return_counts=True)[1]\n",
    "    numerator = np.sum(counts*(counts-1))\n",
    "    n = len(string_list)\n",
    "    if n < 2:\n",
    "        return 0.0\n",
    "    denominator = n*(n-1)\n",
    "    return numerator/denominator\n",
    "\n",
    "def lexical_features(text: str) -> dict:\n",
    "    words = word_tokenize(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    unique_words = set(words)\n",
    "    return {\n",
    "        \"word_count\": len(words),\n",
    "        \"character_count\": sum(len(w) for w in words),\n",
    "        \"average_word_length\": sum(len(w) for w in words) / len(words) if words else 0,\n",
    "        \"sentence_count\": len(sentences),\n",
    "        \"TTR\": len(unique_words) / len(words) if words else 0,\n",
    "        \"RTTR\": np.sqrt(len(unique_words)) / len(words) if words else 0,\n",
    "        \"CTTR\": len(unique_words) / ((len(words)*2) ** 0.5) if words else 0,\n",
    "        \"DMetric\": d_metric(text),\n",
    "        \"Mass\": (np.log10(len(words)) - np.log10(len(unique_words))) / (np.log10(len(words))**2) if len(words) > 1 else 0,\n",
    "        \"stopword_ratio\": len([w for w in words if w.lower() in stop_words]) / len(words) if words else 0,\n",
    "    }\n",
    "\n",
    "\n",
    "def nlp_features(text: str) -> dict:\n",
    "    doc = nlp(text)\n",
    "    pos_counts = Counter(token.pos_ for token in doc)\n",
    "    entities = list(doc.ents)\n",
    "    sentiment_scores = [token.sentiment for token in doc if token.sentiment != 0]\n",
    "    distances = [abs(token.head.i - token.i) for token in doc if token.head != token]\n",
    "    return {\n",
    "        \"noun_ratio\": pos_counts.get(\"NOUN\", 0) / len(doc) if doc else 0,\n",
    "        \"verb_ratio\": pos_counts.get(\"VERB\", 0) / len(doc) if doc else 0,\n",
    "        \"adjective_ratio\": pos_counts.get(\"ADJ\", 0) / len(doc) if doc else 0,\n",
    "        \"average_sentence_length\": sum(len(sent.text.split()) for sent in doc.sents) / len(list(doc.sents)) if list(doc.sents) else 0,\n",
    "        \"std_sentence_length\": statistics.pstdev([len(sent.text.split()) for sent in doc.sents]) if list(doc.sents) else 0,\n",
    "        \"entity_count\": len(entities),\n",
    "        \"syntactic_depth\": max((len(list(token.ancestors)) for token in doc), default=0),\n",
    "        \"dependency_distance\": np.mean(distances) if distances else 0,\n",
    "        \"average_sentiment_score\": np.mean(sentiment_scores) if sentiment_scores else 0,\n",
    "        \"sentiment_variability\": np.std(sentiment_scores) if len(sentiment_scores) > 1 else 0,\n",
    "    }\n",
    "\n",
    "\n",
    "def readability_features(text: str) -> dict:\n",
    "    return {\n",
    "        \"flesch_reading_ease\": textstat.flesch_reading_ease(text),\n",
    "        \"gunning_fog_index\": textstat.gunning_fog(text),\n",
    "        \"smog_index\": textstat.smog_index(text),\n",
    "        \"automated_readability_index\": textstat.automated_readability_index(text),\n",
    "        \"dale_chall_readability\": textstat.dale_chall_readability_score(text),\n",
    "    }\n",
    "\n",
    "\n",
    "def stylometric_features(text: str) -> dict:\n",
    "    words = word_tokenize(text.lower())\n",
    "    return {\n",
    "        \"punctuation_count\": sum(1 for char in text if char in \".,;!?\"),\n",
    "        \"entropy_score\": entropy(list(Counter(words).values())),\n",
    "    }\n",
    "\n",
    "\n",
    "def discourse_features(text: str) -> dict:\n",
    "    markers = {\"however\", \"therefore\", \"moreover\", \"nevertheless\", \"thus\", \"on the other hand\"}\n",
    "    words = word_tokenize(text)\n",
    "    count = sum(1 for w in words if w.lower() in markers)\n",
    "    return {\n",
    "        \"discourse_marker_count\": count,\n",
    "        \"discourse_marker_ratio\": count / len(words) if words else 0,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def repetition_features(text: str) -> dict:\n",
    "    tokens = [w.lower() for w in word_tokenize(text)]\n",
    "    def _ngram(n):\n",
    "        ngrams = list(zip(*(tokens[i:] for i in range(n))))\n",
    "        return 1 - len(set(ngrams)) / len(ngrams) if ngrams else 0\n",
    "    freq = Counter(tokens)\n",
    "    hapax = sum(1 for w,c in freq.items() if c == 1)\n",
    "    return {\n",
    "        \"bigram_repetition_ratio\": _ngram(2),\n",
    "        \"trigram_repetition_ratio\": _ngram(3),\n",
    "        \"hapax_legomena_ratio\": hapax / len(tokens) if tokens else 0,\n",
    "    }\n",
    "\n",
    "\n",
    "def syntactic_features(text: str) -> dict:\n",
    "    return {\n",
    "        \"present_participle_count\": sum(1 for w in word_tokenize(text) if w.lower().endswith('ing')),\n",
    "        \"passive_voice_count\": len(re.findall(r\"\\b(was|were|is|are|been|being)\\s+\\w+ed\\b\", text, flags=re.IGNORECASE)),\n",
    "    }\n",
    "\n",
    "\n",
    "def cohesion_features(text: str) -> dict:\n",
    "    text_l = text.lower()\n",
    "    return {\n",
    "        \"conjunction_count\": sum(text_l.count(w) for w in [\" and \", \" or \", \" but \", \" however \", \" because \", \" therefore \"]),\n",
    "        \"pronoun_count\": sum(text_l.count(p) for p in [\" i \", \" you \", \" he \", \" she \", \" they \", \" we \", \" it \"]),\n",
    "        \"contraction_count\": sum(len(re.findall(p, text)) for p in [r\"\\b\\w+n't\\b\", r\"\\b\\w+'re\\b\", r\"\\b\\w+'ve\\b\", r\"\\b\\w+'ll\\b\", r\"\\b\\w+'d\\b\"]),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def extract_features_single_text(text: str) -> dict:\n",
    "    features = {}\n",
    "    features.update(lexical_features(text))\n",
    "    features.update(nlp_features(text))\n",
    "    features.update(readability_features(text))\n",
    "    features.update(stylometric_features(text))\n",
    "    features.update(discourse_features(text))\n",
    "    features.update(repetition_features(text))\n",
    "    features.update(syntactic_features(text))\n",
    "    features.update(cohesion_features(text))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_features(texts):\n",
    "    results = []\n",
    "    for text in tqdm(texts):\n",
    "        features = extract_features_single_text(text)\n",
    "        results.append(features)\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_paths(folder_path, recursive=False):\n",
    "    if recursive:\n",
    "        # Walk through all subdirectories\n",
    "        file_paths = [os.path.join(root, file) \n",
    "                      for root, _, files in os.walk(folder_path) \n",
    "                      for file in files if file.endswith('.csv')]\n",
    "    else:\n",
    "        # Get files in the root folder only\n",
    "        file_paths = [os.path.join(folder_path, file) \n",
    "                      for file in os.listdir(folder_path) \n",
    "                      if file.endswith('.csv')]\n",
    "    \n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_feature_stats(df, stats, data_path, save_path):\n",
    "    df_stat = df.agg(stats).reset_index()\n",
    "\n",
    "    data_name, model = data_path.split(\"/\")[-1].split(\"_\")\n",
    "    model = model.removesuffix(\".csv\")\n",
    "\n",
    "    df_stat[\"model\"] = model\n",
    "    df_stat[\"data\"] = data_name\n",
    "    df_stat.rename(columns={\"index\": \"stat\"}, inplace=True)\n",
    "    df_stat.to_csv(save_path, mode=\"a\", index=False, header=not pd.io.common.file_exists(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentile(n):\n",
    "    def percentile_(x):\n",
    "        return x.quantile(n)\n",
    "    percentile_.__name__ = 'percentile_{:02.0f}'.format(n*100)\n",
    "    return percentile_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_HUMAN_PATH = \"../data/data_human\"\n",
    "DATA_AI_PATH = \"../data/data_ai\"\n",
    "FEATURES_PATH = \"../data/features/\"\n",
    "FEATURES_STATS_PATH = \"../data/features/features_stats_master.csv\"\n",
    "STATS = ['mean', 'std', 'min', 'max', 'median', 'skew', 'kurtosis', 'var', percentile(0.1), percentile(0.2), percentile(0.3), percentile(0.4), percentile(0.5), percentile(0.6), percentile(0.7), percentile(0.8), percentile(0.9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = get_csv_paths(DATA_HUMAN_PATH) + get_csv_paths(DATA_AI_PATH, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11580/11580 [03:40<00:00, 52.54it/s]\n",
      "100%|██████████| 11581/11581 [11:12<00:00, 17.21it/s] \n",
      "  5%|▌         | 592/11581 [00:10<04:18, 42.57it/s]"
     ]
    }
   ],
   "source": [
    "for path in paths:\n",
    "    if path.split(\"_\")[-1] == \"human.csv\":\n",
    "        features_path = os.path.join(FEATURES_PATH, path.split(\"/\")[-2], path.split(\"/\")[-1].replace(\".csv\", \"_features.csv\"))\n",
    "    else:\n",
    "        features_path = os.path.join(FEATURES_PATH, path.split(\"/\")[-3], path.split(\"/\")[-2], path.split(\"/\")[-1].replace(\".csv\", \"_features.csv\"))\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    texts = df[\"text\"].values\n",
    "    df_features = calc_features(texts)\n",
    "    # df_features.to_csv(features_path, index=False)\n",
    "\n",
    "    # save_feature_stats(df_features, STATS, path, FEATURES_STATS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
