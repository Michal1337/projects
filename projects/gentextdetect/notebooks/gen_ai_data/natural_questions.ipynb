{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-03 00:07:33 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "SEED = 1337\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ef1ec44eae45d7ab7573e2c0ac6bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30103467e0da4fc7b465bdb4ba336f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77df1f712c304a59a706736d310c9322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'document', 'question', 'long_answer_candidates', 'annotations'],\n",
       "        num_rows: 307373\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'document', 'question', 'long_answer_candidates', 'annotations'],\n",
       "        num_rows: 7830\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"google-research-datasets/natural_questions\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../data/data_raw/natural_questions.csv\"\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(path, mode='w', newline='', encoding='utf-8') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow([\"document\", \"question\", \"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 219702/307373 [1:06:07<26:36, 54.92it/s]  "
     ]
    }
   ],
   "source": [
    "documents, questions, answers = [], [], []\n",
    "\n",
    "for item in tqdm(dataset[\"train\"]):\n",
    "    idx = np.random.randint(len(item[\"long_answer_candidates\"][\"start_token\"]))\n",
    "    start = item[\"long_answer_candidates\"][\"start_token\"][idx]\n",
    "    end = item[\"long_answer_candidates\"][\"end_token\"][idx]\n",
    "    tokens = item[\"document\"][\"tokens\"]\n",
    "\n",
    "    question = \" \".join(token for token in item[\"question\"][\"tokens\"])\n",
    "\n",
    "    ans = tokens[\"token\"][start:end]\n",
    "    ans_is_html = tokens[\"is_html\"][start:end]\n",
    "    ans = \" \".join([token for token, html in zip(ans, ans_is_html) if not html])\n",
    "\n",
    "    doc_is_html = tokens[\"is_html\"]\n",
    "    document = \" \".join([token for token, html in zip(tokens[\"token\"], doc_is_html) if not html])\n",
    "\n",
    "    documents.append(document)\n",
    "    questions.append(question)\n",
    "    answers.append(ans)\n",
    "    if len(documents) == batch_size:\n",
    "        with open(path, mode='a', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            for document, question, answer in zip(documents, questions, answers):\n",
    "                writer.writerow([document, question, answer])\n",
    "\n",
    "        documents, questions, answers = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(173, 173, 173)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents), len(questions), len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path, mode='a', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for document, question, answer in zip(documents, questions, answers):\n",
    "        writer.writerow([document, question, answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7830/7830 [03:07<00:00, 41.79it/s] \n"
     ]
    }
   ],
   "source": [
    "documents, questions, answers = [], [], []\n",
    "\n",
    "for item in tqdm(dataset[\"validation\"]):\n",
    "    idx = np.random.randint(len(item[\"long_answer_candidates\"][\"start_token\"]))\n",
    "    start = item[\"long_answer_candidates\"][\"start_token\"][idx]\n",
    "    end = item[\"long_answer_candidates\"][\"end_token\"][idx]\n",
    "    tokens = item[\"document\"][\"tokens\"]\n",
    "\n",
    "    question = \" \".join(token for token in item[\"question\"][\"tokens\"])\n",
    "\n",
    "    ans = tokens[\"token\"][start:end]\n",
    "    ans_is_html = tokens[\"is_html\"][start:end]\n",
    "    ans = \" \".join([token for token, html in zip(ans, ans_is_html) if not html])\n",
    "\n",
    "    doc_is_html = tokens[\"is_html\"]\n",
    "    document = \" \".join([token for token, html in zip(tokens[\"token\"], doc_is_html) if not html])\n",
    "\n",
    "    documents.append(document)\n",
    "    questions.append(question)\n",
    "    answers.append(ans)\n",
    "    if len(documents) == batch_size:\n",
    "        with open(path, mode='a', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            for document, question, answer in zip(documents, questions, answers):\n",
    "                writer.writerow([document, question, answer])\n",
    "\n",
    "        documents, questions, answers = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 150, 150)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents), len(questions), len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path, mode='a', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for document, question, answer in zip(documents, questions, answers):\n",
    "        writer.writerow([document, question, answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(iterable, batch_size):\n",
    "    \"\"\"Splits an iterable into smaller batches.\"\"\"\n",
    "    iterable = iter(iterable)\n",
    "    while batch := list(islice(iterable, batch_size)):\n",
    "        yield batch\n",
    "\n",
    "def save_to_csv(path, prompts, responses, temperature, top_p, top_k):\n",
    "    \"\"\"Saves prompts, responses and sampling parameters to a CSV file.\"\"\"\n",
    "    with open(path, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for prompt, response in zip(prompts, responses):\n",
    "            writer.writerow([prompt, response, temperature, top_p, top_k])\n",
    "\n",
    "def generate_responses(model, prompts, sampling_params):\n",
    "    \"\"\"Generate a batch of outputs using vLLM with customizable sampling parameters.\"\"\"\n",
    "    outputs = model.chat(prompts, sampling_params=sampling_params, use_tqdm=False)\n",
    "    \n",
    "    return [sample.outputs[0].text.replace('\"', '') for sample in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PROMPT = [{\"role\": \"system\", \"content\": \"You are a helpful assistant for answering questions based on provided context. Based on provided context, which will be in a form of a copy of wikipedia article, answer the question. MAKE SURE TO REPLAY ONLY WITH THE ANSWER.\"},\n",
    "                {\"role\": \"user\", \"content\": \"Context: \\n {context} \\n Question: {question}\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"Answer: \\n\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [04:26,  3.38s/it]                        \n",
      "79it [04:22,  3.33s/it]                        \n",
      "79it [04:21,  3.30s/it]                        \n",
      "79it [03:55,  2.98s/it]                        \n",
      "79it [03:48,  2.89s/it]                        \n",
      "79it [04:17,  3.26s/it]                        \n",
      "79it [04:19,  3.28s/it]                        \n",
      "79it [04:16,  3.25s/it]                        \n",
      "79it [04:26,  3.38s/it]                        \n",
      "79it [03:45,  2.85s/it]                        \n",
      "79it [04:24,  3.35s/it]                        \n",
      "79it [04:28,  3.40s/it]                        \n",
      "79it [04:35,  3.49s/it]                        \n",
      "79it [04:33,  3.47s/it]                        \n",
      "79it [04:27,  3.39s/it]                        \n",
      "79it [04:10,  3.17s/it]                        \n",
      "79it [03:53,  2.96s/it]                        \n",
      "79it [04:05,  3.11s/it]                        \n",
      "79it [03:53,  2.95s/it]                        \n",
      "79it [03:53,  2.96s/it]                        \n",
      "79it [02:57,  2.25s/it]                        \n",
      "79it [02:50,  2.16s/it]                        \n",
      "79it [02:51,  2.18s/it]                        \n",
      "79it [02:48,  2.13s/it]                        \n",
      "79it [02:48,  2.13s/it]                        \n",
      "79it [02:41,  2.05s/it]                        \n",
      "79it [02:48,  2.14s/it]                        \n",
      "79it [02:54,  2.21s/it]                        \n",
      "79it [02:50,  2.16s/it]                        \n",
      "79it [02:51,  2.17s/it]                        \n",
      "79it [02:46,  2.11s/it]                        \n",
      "41it [01:24,  2.05s/it]                        \n"
     ]
    }
   ],
   "source": [
    "tl1, tl2, tl3, tl4, tl5 = [], [], [], [], []\n",
    "for df in pd.read_csv(path, chunksize=10_000):\n",
    "    prompts = [\n",
    "    [\n",
    "        BASE_PROMPT[0],  # The system message\n",
    "        {\"role\": \"user\", \"content\": BASE_PROMPT[1][\"content\"].format(context=context, question=question)},  # Formatted user message\n",
    "        BASE_PROMPT[2]  # The assistant message\n",
    "    ]\n",
    "    for context, question in df[[\"document\", \"question\"]].values\n",
    "    ]\n",
    "    lens = []\n",
    "    batch_size = 128\n",
    "    for prompts_batch in tqdm(batchify(prompts, batch_size), total=len(prompts) // batch_size):\n",
    "        tokens = tokenizer.apply_chat_template(prompts_batch)\n",
    "        lens.extend([len(token) for token in tokens])\n",
    "    too_large1 = [i for i, l in enumerate(lens) if l > 32_768]\n",
    "    too_large2 = [i for i, l in enumerate(lens) if l > 32_768 * 2]\n",
    "    too_large3 = [i for i, l in enumerate(lens) if l > 32_768 * 3]\n",
    "    too_large4 = [i for i, l in enumerate(lens) if l > 32_768 * 1/2]\n",
    "    too_large5 = [i for i, l in enumerate(lens) if l > 32_768 * 1/4]\n",
    "\n",
    "    tl1.extend(too_large1)\n",
    "    tl2.extend(too_large2)\n",
    "    tl3.extend(too_large3)\n",
    "    tl4.extend(too_large4)\n",
    "    tl5.extend(too_large5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9038, 368, 14, 53149, 126596)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tl1), len(tl2), len(tl3), len(tl4), len(tl5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"document\", \"question\"], axis=1, inplace=True)\n",
    "df.to_csv(\"../../data/data_human/natural_questions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = [\n",
    "    SamplingParams(temperature=0.0, top_p=1.0, top_k=-1, max_tokens=30_000, seed=SEED),  # Pure Greedy (fully deterministic)\n",
    "    SamplingParams(temperature=0.2, top_p=1.0, top_k=-1, max_tokens=30_000, seed=SEED),  # Highly Deterministic\n",
    "    SamplingParams(temperature=0.5, top_p=0.95, top_k=100, max_tokens=30_000, seed=SEED), # Mildly Deterministic but Flexible\n",
    "    SamplingParams(temperature=0.7, top_p=0.9, top_k=50, max_tokens=30_000, seed=SEED),  # Balanced and Natural\n",
    "    SamplingParams(temperature=0.9, top_p=0.8, top_k=40, max_tokens=30_000, seed=SEED),  # Slightly More Diverse but Coherent\n",
    "    SamplingParams(temperature=1.0, top_p=0.95, top_k=30, max_tokens=30_000, seed=SEED), # Default Creative Mode\n",
    "    SamplingParams(temperature=1.2, top_p=0.7, top_k=20, max_tokens=30_000, seed=SEED),  # Highly Creative\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llms = [\"meta-llama/Llama-3.2-1B-Instruct\"]\n",
    "batch_size = 8\n",
    "base_path = \"../../data/data_ai/natural_questions/natural-questions_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 02-15 19:39:32 config.py:2386] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 02-15 19:39:43 config.py:542] This model supports multiple tasks: {'embed', 'score', 'generate', 'reward', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 02-15 19:39:43 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='meta-llama/Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=10000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 02-15 19:39:45 interface.py:284] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 02-15 19:39:45 cuda.py:179] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 02-15 19:39:45 cuda.py:227] Using XFormers backend.\n",
      "INFO 02-15 19:39:46 model_runner.py:1110] Starting to load model meta-llama/Llama-3.2-1B-Instruct...\n",
      "INFO 02-15 19:39:47 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
      "INFO 02-15 19:39:47 weight_utils.py:297] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e3bd7e88d646a796b2b5a338795733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-15 19:40:13 model_runner.py:1115] Loading model weights took 2.3185 GB\n",
      "INFO 02-15 19:40:16 worker.py:267] Memory profiling takes 2.28 seconds\n",
      "INFO 02-15 19:40:16 worker.py:267] the current vLLM instance can use total_gpu_memory (6.00GiB) x gpu_memory_utilization (0.90) = 5.40GiB\n",
      "INFO 02-15 19:40:16 worker.py:267] model weights take 2.32GiB; non_torch_memory takes 0.04GiB; PyTorch activation peak memory takes 1.21GiB; the rest of the memory reserved for KV Cache is 1.84GiB.\n",
      "INFO 02-15 19:40:16 executor_base.py:110] # CUDA blocks: 3761, # CPU blocks: 8192\n",
      "INFO 02-15 19:40:16 executor_base.py:115] Maximum concurrency for 10000 tokens per request: 6.02x\n",
      "INFO 02-15 19:41:00 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:32<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-15 19:41:33 model_runner.py:1562] Graph capturing finished in 33 secs, took 0.12 GiB\n",
      "INFO 02-15 19:41:33 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 79.90 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/28338 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-15 19:41:33 chat_utils.py:332] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/28338 [00:13<52:26:24,  6.66s/it]\n"
     ]
    }
   ],
   "source": [
    "for llm in llms:\n",
    "    model = LLM(model=llm, dtype=\"half\", max_model_len = 10_000)\n",
    "    csv_path = f\"{base_path}{llm.split('/')[-1]}.csv\"\n",
    "\n",
    "\n",
    "    # init csv file\n",
    "    with open(csv_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"prompt\", \"response\", \"temperature\", \"top_p\", \"top_k\"])\n",
    "\n",
    "    cnt = 0\n",
    "    for prompts_batch in tqdm(batchify(prompts, batch_size), total=len(prompts) // batch_size):\n",
    "        params = random.choice(sampling_params)\n",
    "        responses = generate_responses(model, prompts_batch, params)\n",
    "        save_to_csv(csv_path, prompts_batch, responses, params.temperature, params.top_p, params.top_k)\n",
    "        cnt += 1\n",
    "        if cnt > 2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>top_k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a help...</td>\n",
       "      <td>**Clean-up Operations Continue After Storm Fra...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a help...</td>\n",
       "      <td>**TRAGEDY STRIKES Belfast City Centre: Tourist...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a help...</td>\n",
       "      <td>LEWIS HAMILTON SENSATION AS HAMILTON TAKES OVE...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a help...</td>\n",
       "      <td>I cannot generate an article that contains exp...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a help...</td>\n",
       "      <td>ISTANBUL, TURKEY - A 35-year-old man who was b...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  [{'role': 'system', 'content': 'You are a help...   \n",
       "1  [{'role': 'system', 'content': 'You are a help...   \n",
       "2  [{'role': 'system', 'content': 'You are a help...   \n",
       "3  [{'role': 'system', 'content': 'You are a help...   \n",
       "4  [{'role': 'system', 'content': 'You are a help...   \n",
       "\n",
       "                                            response  temperature  top_p  \\\n",
       "0  **Clean-up Operations Continue After Storm Fra...          1.2    0.7   \n",
       "1  **TRAGEDY STRIKES Belfast City Centre: Tourist...          1.2    0.7   \n",
       "2  LEWIS HAMILTON SENSATION AS HAMILTON TAKES OVE...          1.2    0.7   \n",
       "3  I cannot generate an article that contains exp...          1.2    0.7   \n",
       "4  ISTANBUL, TURKEY - A 35-year-old man who was b...          1.2    0.7   \n",
       "\n",
       "   top_k  \n",
       "0     20  \n",
       "1     20  \n",
       "2     20  \n",
       "3     20  \n",
       "4     20  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_csv(\"../../data/data_ai/natural_questions/natural-questions_Llama-3.2-1B-Instruct.csv\")\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
