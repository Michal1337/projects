{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing different neural network architectures for next-token-prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Micha≈Ç Gromadzki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.random.set_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using tiny shakespeare as input data\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda s: [stoi[c] for c in s] \n",
    "decode = lambda l: ''.join([itos[i] for i in l]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(text))\n",
    "train_data = encode(text[:n])\n",
    "val_data = encode(text[n:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(train_data)\n",
    "val_ds = tf.data.Dataset.from_tensor_slices(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to d\n"
     ]
    }
   ],
   "source": [
    "block_size = 128\n",
    "sequences_train = train_ds.batch(block_size+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences_train.take(1):\n",
    "  print(decode(seq.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_val = val_ds.batch(block_size+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequence(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('abcdefg', 'bcdefgh')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_sequence(\"abcdefgh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_final = sequences_train.map(split_sequence)\n",
    "val_ds_final = sequences_val.map(split_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ds(text, block_size, batch_size):\n",
    "    n = int(0.9*len(text))\n",
    "    train_data = encode(text[:n])\n",
    "    val_data = encode(text[n:])\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices(train_data)\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices(val_data)\n",
    "    sequences_train = train_ds.batch(block_size+1, drop_remainder=True)\n",
    "    sequences_val = val_ds.batch(block_size+1, drop_remainder=True)\n",
    "    train_ds_final = sequences_train.map(split_sequence)\n",
    "    val_ds_final = sequences_val.map(split_sequence)\n",
    "    return train_ds_final.batch(batch_size), val_ds_final.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, block_size, context, new_tokens):\n",
    "    context = tf.expand_dims(context[0,-block_size:],0)\n",
    "    for i in range(new_tokens):\n",
    "        logits = model.predict(tf.expand_dims(context[0,-block_size:],0),verbose=0)\n",
    "        logits = logits[0,-1,:]\n",
    "        pred = tf.random.categorical(tf.math.log(tf.expand_dims(logits,0)),1)\n",
    "        pred = tf.cast(pred,dtype=tf.int32)\n",
    "        context = tf.concat([context,pred],axis=1)\n",
    "    return context"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense - small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens = 100 #new tokens to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 32 #context size\n",
    "batch_size = 32\n",
    "n_embed = 4 #embedding dimensions of each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = make_ds(text, block_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, n_embed, input_length=block_size),\n",
    "    tf.keras.layers.Dense(vocab_size),\n",
    "    tf.keras.layers.Softmax(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 32, 4)             260       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32, 65)            325       \n",
      "                                                                 \n",
      " softmax_2 (Softmax)         (None, 32, 65)            0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 585\n",
      "Trainable params: 585\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.losses.SparseCategoricalCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "951/951 [==============================] - 6s 6ms/step - loss: 3.3209 - val_loss: 2.8967\n",
      "Epoch 2/20\n",
      "951/951 [==============================] - 6s 6ms/step - loss: 2.7704 - val_loss: 2.6993\n",
      "Epoch 3/20\n",
      "951/951 [==============================] - 6s 6ms/step - loss: 2.6690 - val_loss: 2.6453\n",
      "Epoch 4/20\n",
      "951/951 [==============================] - 6s 6ms/step - loss: 2.6365 - val_loss: 2.6239\n",
      "Epoch 5/20\n",
      "951/951 [==============================] - 6s 6ms/step - loss: 2.6211 - val_loss: 2.6130\n",
      "Epoch 6/20\n",
      "951/951 [==============================] - 6s 6ms/step - loss: 2.6127 - val_loss: 2.6067\n",
      "Epoch 7/20\n",
      "951/951 [==============================] - 6s 6ms/step - loss: 2.6077 - val_loss: 2.6026\n",
      "Epoch 8/20\n",
      "951/951 [==============================] - 5s 6ms/step - loss: 2.6043 - val_loss: 2.5997\n",
      "Epoch 9/20\n",
      "951/951 [==============================] - 6s 6ms/step - loss: 2.6019 - val_loss: 2.5974\n",
      "Epoch 10/20\n",
      "951/951 [==============================] - 6s 6ms/step - loss: 2.6000 - val_loss: 2.5957\n",
      "Epoch 11/20\n",
      "951/951 [==============================] - 6s 6ms/step - loss: 2.5985 - val_loss: 2.5942\n",
      "Epoch 12/20\n",
      "951/951 [==============================] - 6s 6ms/step - loss: 2.5972 - val_loss: 2.5930\n",
      "Epoch 13/20\n",
      "951/951 [==============================] - 6s 6ms/step - loss: 2.5962 - val_loss: 2.5920\n",
      "Epoch 14/20\n",
      "951/951 [==============================] - 6s 6ms/step - loss: 2.5953 - val_loss: 2.5911\n",
      "Epoch 15/20\n",
      "951/951 [==============================] - 6s 6ms/step - loss: 2.5946 - val_loss: 2.5904\n",
      "Epoch 16/20\n",
      "951/951 [==============================] - 6s 6ms/step - loss: 2.5939 - val_loss: 2.5897\n",
      "Epoch 17/20\n",
      "951/951 [==============================] - 6s 6ms/step - loss: 2.5934 - val_loss: 2.5891\n",
      "Epoch 18/20\n",
      "951/951 [==============================] - 6s 6ms/step - loss: 2.5928 - val_loss: 2.5886\n",
      "Epoch 19/20\n",
      "951/951 [==============================] - 6s 6ms/step - loss: 2.5924 - val_loss: 2.5881\n",
      "Epoch 20/20\n",
      "951/951 [==============================] - 6s 7ms/step - loss: 2.5919 - val_loss: 2.5877\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21902f4b610>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, epochs=20, batch_size=batch_size, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "\n",
      "GREMIO:\n",
      "Good morrow, neighbou\n",
      "--------------------\n",
      "?\n",
      "\n",
      "GREMIO:\n",
      "Good morrow, neighboue'dy ar thbe sh s wl moive war ingutheereot STy wiy? alt\n",
      "IXCENEENEBuur punkiwaeppe pou, alee arth, s\n"
     ]
    }
   ],
   "source": [
    "for context, _ in val_ds.take(1):    \n",
    "    print(decode(context[0].numpy()))\n",
    "    result = generate(model, block_size , context, new_tokens)\n",
    "    print(\"-\" * 20)\n",
    "    print(decode(result.numpy()[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense - large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 64\n",
    "batch_size = 32\n",
    "n_embed = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = make_ds(text, block_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, n_embed, input_length=block_size),\n",
    "    tf.keras.layers.Dense(block_size * n_embed),\n",
    "    tf.keras.layers.Dense(128),\n",
    "    tf.keras.layers.Dense(128),\n",
    "    tf.keras.layers.Dense(vocab_size),\n",
    "    tf.keras.layers.Softmax(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.losses.SparseCategoricalCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 64, 32)            2080      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64, 2048)          67584     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64, 128)           262272    \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64, 128)           16512     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64, 65)            8385      \n",
      "                                                                 \n",
      " softmax_3 (Softmax)         (None, 64, 65)            0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 356,833\n",
      "Trainable params: 356,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "483/483 [==============================] - 8s 16ms/step - loss: 2.5979 - val_loss: 2.5012\n",
      "Epoch 2/20\n",
      "483/483 [==============================] - 7s 15ms/step - loss: 2.4882 - val_loss: 2.4950\n",
      "Epoch 3/20\n",
      "483/483 [==============================] - 8s 16ms/step - loss: 2.4809 - val_loss: 2.4951\n",
      "Epoch 4/20\n",
      "483/483 [==============================] - 8s 16ms/step - loss: 2.4778 - val_loss: 2.4943\n",
      "Epoch 5/20\n",
      "483/483 [==============================] - 8s 17ms/step - loss: 2.4758 - val_loss: 2.4934\n",
      "Epoch 6/20\n",
      "483/483 [==============================] - 8s 17ms/step - loss: 2.4744 - val_loss: 2.4924\n",
      "Epoch 7/20\n",
      "483/483 [==============================] - 8s 17ms/step - loss: 2.4732 - val_loss: 2.4913\n",
      "Epoch 8/20\n",
      "483/483 [==============================] - 8s 17ms/step - loss: 2.4723 - val_loss: 2.4903\n",
      "Epoch 9/20\n",
      "483/483 [==============================] - 8s 17ms/step - loss: 2.4715 - val_loss: 2.4897\n",
      "Epoch 10/20\n",
      "483/483 [==============================] - 8s 17ms/step - loss: 2.4708 - val_loss: 2.4893\n",
      "Epoch 11/20\n",
      "483/483 [==============================] - 8s 17ms/step - loss: 2.4702 - val_loss: 2.4889\n",
      "Epoch 12/20\n",
      "483/483 [==============================] - 9s 18ms/step - loss: 2.4696 - val_loss: 2.4886\n",
      "Epoch 13/20\n",
      "483/483 [==============================] - 8s 16ms/step - loss: 2.4690 - val_loss: 2.4881\n",
      "Epoch 14/20\n",
      "483/483 [==============================] - 8s 16ms/step - loss: 2.4684 - val_loss: 2.4877\n",
      "Epoch 15/20\n",
      "483/483 [==============================] - 8s 17ms/step - loss: 2.4679 - val_loss: 2.4871\n",
      "Epoch 16/20\n",
      "483/483 [==============================] - 8s 17ms/step - loss: 2.4675 - val_loss: 2.4866\n",
      "Epoch 17/20\n",
      "483/483 [==============================] - 8s 17ms/step - loss: 2.4670 - val_loss: 2.4860\n",
      "Epoch 18/20\n",
      "483/483 [==============================] - 8s 16ms/step - loss: 2.4665 - val_loss: 2.4851\n",
      "Epoch 19/20\n",
      "483/483 [==============================] - 8s 16ms/step - loss: 2.4660 - val_loss: 2.4845\n",
      "Epoch 20/20\n",
      "483/483 [==============================] - 7s 15ms/step - loss: 2.4653 - val_loss: 2.4834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21a6fe1df90>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, epochs=20, batch_size=batch_size, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "\n",
      "GREMIO:\n",
      "Good morrow, neighbour Baptista.\n",
      "\n",
      "BAPTISTA:\n",
      "Good morr\n",
      "--------------------\n",
      "?\n",
      "\n",
      "GREMIO:\n",
      "Good morrow, neighbour Baptista.\n",
      "\n",
      "BAPTISTA:\n",
      "Good morrane wearknon son doocyo w pis jourel t y louthmy an.\n",
      "TIO sio, es hetou ioil ll VIO:\n",
      "TIIneter\n",
      "Pllo ck\n"
     ]
    }
   ],
   "source": [
    "for context, _ in val_ds.take(1):    \n",
    "    print(decode(context[0].numpy()))\n",
    "    result = generate(model, block_size , context, new_tokens)\n",
    "    print(\"-\" * 20)\n",
    "    print(decode(result.numpy()[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN - small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 32\n",
    "batch_size = 32\n",
    "n_embed = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = make_ds(text, block_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, n_embed, input_length=block_size),\n",
    "    tf.keras.layers.SimpleRNN(block_size * n_embed, return_sequences=True),\n",
    "    tf.keras.layers.Dense(vocab_size),\n",
    "    tf.keras.layers.Softmax(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.losses.SparseCategoricalCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 32, 4)             260       \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 32, 128)           17024     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 32, 65)            8385      \n",
      "                                                                 \n",
      " softmax_4 (Softmax)         (None, 32, 65)            0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,669\n",
      "Trainable params: 25,669\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "951/951 [==============================] - 50s 51ms/step - loss: 2.8124 - val_loss: 2.4658\n",
      "Epoch 2/20\n",
      "951/951 [==============================] - 47s 49ms/step - loss: 2.3502 - val_loss: 2.2999\n",
      "Epoch 3/20\n",
      "951/951 [==============================] - 56s 59ms/step - loss: 2.2176 - val_loss: 2.2179\n",
      "Epoch 4/20\n",
      "951/951 [==============================] - 66s 70ms/step - loss: 2.1332 - val_loss: 2.1611\n",
      "Epoch 5/20\n",
      "951/951 [==============================] - 46s 49ms/step - loss: 2.0678 - val_loss: 2.1177\n",
      "Epoch 6/20\n",
      "951/951 [==============================] - 45s 47ms/step - loss: 2.0155 - val_loss: 2.0816\n",
      "Epoch 7/20\n",
      "951/951 [==============================] - 45s 48ms/step - loss: 1.9733 - val_loss: 2.0512\n",
      "Epoch 8/20\n",
      "951/951 [==============================] - 46s 48ms/step - loss: 1.9385 - val_loss: 2.0259\n",
      "Epoch 9/20\n",
      "951/951 [==============================] - 51s 54ms/step - loss: 1.9097 - val_loss: 2.0052\n",
      "Epoch 10/20\n",
      "951/951 [==============================] - 67s 70ms/step - loss: 1.8857 - val_loss: 1.9880\n",
      "Epoch 11/20\n",
      "951/951 [==============================] - 64s 67ms/step - loss: 1.8654 - val_loss: 1.9740\n",
      "Epoch 12/20\n",
      "951/951 [==============================] - 65s 68ms/step - loss: 1.8481 - val_loss: 1.9621\n",
      "Epoch 13/20\n",
      "951/951 [==============================] - 64s 68ms/step - loss: 1.8333 - val_loss: 1.9525\n",
      "Epoch 14/20\n",
      "951/951 [==============================] - 64s 67ms/step - loss: 1.8202 - val_loss: 1.9444\n",
      "Epoch 15/20\n",
      "951/951 [==============================] - 66s 69ms/step - loss: 1.8086 - val_loss: 1.9373\n",
      "Epoch 16/20\n",
      "951/951 [==============================] - 62s 65ms/step - loss: 1.7982 - val_loss: 1.9306\n",
      "Epoch 17/20\n",
      "951/951 [==============================] - 62s 66ms/step - loss: 1.7889 - val_loss: 1.9245\n",
      "Epoch 18/20\n",
      "951/951 [==============================] - 64s 67ms/step - loss: 1.7805 - val_loss: 1.9191\n",
      "Epoch 19/20\n",
      "951/951 [==============================] - 64s 67ms/step - loss: 1.7729 - val_loss: 1.9138\n",
      "Epoch 20/20\n",
      "951/951 [==============================] - 64s 67ms/step - loss: 1.7661 - val_loss: 1.9089\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21a86d0db10>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, epochs=20, batch_size=batch_size, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "\n",
      "GREMIO:\n",
      "Good morrow, neighbou\n",
      "--------------------\n",
      "?\n",
      "\n",
      "GREMIO:\n",
      "Good morrow, neighboursel:\n",
      "And you whon a quacr goo.\n",
      "\n",
      "RUCENTIO:\n",
      "I gravent you, Basag:\n",
      "Af han mero.\n",
      "\n",
      "HORTENSIO:\n",
      "Af o't way\n"
     ]
    }
   ],
   "source": [
    "for context, _ in val_ds.take(1):    \n",
    "    print(decode(context[0].numpy()))\n",
    "    result = generate(model, block_size , context, new_tokens)\n",
    "    print(\"-\" * 20)\n",
    "    print(decode(result.numpy()[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN - large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 64\n",
    "batch_size = 32\n",
    "n_embed = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = make_ds(text, block_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, n_embed, input_length=block_size),\n",
    "    tf.keras.layers.SimpleRNN(block_size * n_embed, return_sequences=True),\n",
    "    tf.keras.layers.Dense(vocab_size),\n",
    "    tf.keras.layers.Softmax(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_8 (Embedding)     (None, 64, 32)            2080      \n",
      "                                                                 \n",
      " simple_rnn_4 (SimpleRNN)    (None, 64, 2048)          4261888   \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 64, 65)            133185    \n",
      "                                                                 \n",
      " softmax_7 (Softmax)         (None, 64, 65)            0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,397,153\n",
      "Trainable params: 4,397,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.losses.SparseCategoricalCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "483/483 [==============================] - 226s 464ms/step - loss: 3.1819 - val_loss: 2.6138\n",
      "Epoch 2/20\n",
      "483/483 [==============================] - 234s 484ms/step - loss: 2.4503 - val_loss: 2.3319\n",
      "Epoch 3/20\n",
      "483/483 [==============================] - 231s 478ms/step - loss: 2.3483 - val_loss: 2.2622\n",
      "Epoch 4/20\n",
      "483/483 [==============================] - 256s 531ms/step - loss: 2.1937 - val_loss: 2.1953\n",
      "Epoch 5/20\n",
      "483/483 [==============================] - 266s 550ms/step - loss: 2.1233 - val_loss: 2.1463\n",
      "Epoch 6/20\n",
      "483/483 [==============================] - 264s 547ms/step - loss: 2.0727 - val_loss: 2.1187\n",
      "Epoch 7/20\n",
      "483/483 [==============================] - 252s 522ms/step - loss: 2.0559 - val_loss: 2.1020\n",
      "Epoch 8/20\n",
      "483/483 [==============================] - 250s 517ms/step - loss: 2.0051 - val_loss: 2.0855\n",
      "Epoch 9/20\n",
      "483/483 [==============================] - 244s 506ms/step - loss: 1.9793 - val_loss: 2.0674\n",
      "Epoch 10/20\n",
      "483/483 [==============================] - 325s 673ms/step - loss: 1.9560 - val_loss: 2.0618\n",
      "Epoch 11/20\n",
      "483/483 [==============================] - 336s 695ms/step - loss: 1.9383 - val_loss: 2.0463\n",
      "Epoch 12/20\n",
      "483/483 [==============================] - 239s 494ms/step - loss: 1.9236 - val_loss: 2.0316\n",
      "Epoch 13/20\n",
      "483/483 [==============================] - 254s 525ms/step - loss: 1.9249 - val_loss: 2.0202\n",
      "Epoch 14/20\n",
      "483/483 [==============================] - 236s 488ms/step - loss: 1.8972 - val_loss: 2.0155\n",
      "Epoch 15/20\n",
      "483/483 [==============================] - 250s 518ms/step - loss: 1.8880 - val_loss: 2.0104\n",
      "Epoch 16/20\n",
      "483/483 [==============================] - 250s 517ms/step - loss: 1.8822 - val_loss: 2.0071\n",
      "Epoch 17/20\n",
      "483/483 [==============================] - 255s 528ms/step - loss: 1.8725 - val_loss: 1.9996\n",
      "Epoch 18/20\n",
      "483/483 [==============================] - 250s 517ms/step - loss: 1.8651 - val_loss: 2.0092\n",
      "Epoch 19/20\n",
      "483/483 [==============================] - 245s 507ms/step - loss: 1.8617 - val_loss: 2.0066\n",
      "Epoch 20/20\n",
      "483/483 [==============================] - 142s 293ms/step - loss: 1.8502 - val_loss: 2.0080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2192e37e410>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, epochs=20, batch_size=batch_size, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "\n",
      "GREMIO:\n",
      "Good morrow, neighbour Baptista.\n",
      "\n",
      "BAPTISTA:\n",
      "Good morr\n",
      "--------------------\n",
      "?\n",
      "\n",
      "GREMIO:\n",
      "Good morrow, neighbour Baptista.\n",
      "\n",
      "BAPTISTA:\n",
      "Good morr and heresher eny I lovohen!\n",
      "\n",
      "WANUEN RORTER:\n",
      "O the clesin houn I has!\n",
      "That in thee.\n",
      "\n",
      "ESAUNETINSES:\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "for context, _ in val_ds.take(1):    \n",
    "    print(decode(context[0].numpy()))\n",
    "    result = generate(model, block_size , context, new_tokens)\n",
    "    print(\"-\" * 20)\n",
    "    print(decode(result.numpy()[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU - small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 32\n",
    "batch_size = 32\n",
    "n_embed = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = make_ds(text, block_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, n_embed, input_length=block_size),\n",
    "    tf.keras.layers.GRU(block_size * n_embed, return_sequences=True),\n",
    "    tf.keras.layers.Dense(vocab_size),\n",
    "    tf.keras.layers.Softmax(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.losses.SparseCategoricalCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_10 (Embedding)    (None, 32, 4)             260       \n",
      "                                                                 \n",
      " gru_3 (GRU)                 (None, 32, 128)           51456     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 32, 65)            8385      \n",
      "                                                                 \n",
      " softmax_9 (Softmax)         (None, 32, 65)            0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,101\n",
      "Trainable params: 60,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "951/951 [==============================] - 12s 11ms/step - loss: 2.7667 - val_loss: 2.3554\n",
      "Epoch 2/20\n",
      "951/951 [==============================] - 9s 10ms/step - loss: 2.2357 - val_loss: 2.1741\n",
      "Epoch 3/20\n",
      "951/951 [==============================] - 9s 10ms/step - loss: 2.0746 - val_loss: 2.0755\n",
      "Epoch 4/20\n",
      "951/951 [==============================] - 10s 11ms/step - loss: 1.9688 - val_loss: 2.0132\n",
      "Epoch 5/20\n",
      "951/951 [==============================] - 9s 9ms/step - loss: 1.8955 - val_loss: 1.9676\n",
      "Epoch 6/20\n",
      "951/951 [==============================] - 9s 10ms/step - loss: 1.8420 - val_loss: 1.9336\n",
      "Epoch 7/20\n",
      "951/951 [==============================] - 9s 10ms/step - loss: 1.8009 - val_loss: 1.9089\n",
      "Epoch 8/20\n",
      "951/951 [==============================] - 9s 9ms/step - loss: 1.7685 - val_loss: 1.8895\n",
      "Epoch 9/20\n",
      "951/951 [==============================] - 10s 11ms/step - loss: 1.7423 - val_loss: 1.8735\n",
      "Epoch 10/20\n",
      "951/951 [==============================] - 11s 12ms/step - loss: 1.7208 - val_loss: 1.8607\n",
      "Epoch 11/20\n",
      "951/951 [==============================] - 12s 13ms/step - loss: 1.7029 - val_loss: 1.8507\n",
      "Epoch 12/20\n",
      "951/951 [==============================] - 12s 12ms/step - loss: 1.6875 - val_loss: 1.8422\n",
      "Epoch 13/20\n",
      "951/951 [==============================] - 12s 12ms/step - loss: 1.6743 - val_loss: 1.8349\n",
      "Epoch 14/20\n",
      "951/951 [==============================] - 12s 13ms/step - loss: 1.6627 - val_loss: 1.8285\n",
      "Epoch 15/20\n",
      "951/951 [==============================] - 12s 13ms/step - loss: 1.6526 - val_loss: 1.8231\n",
      "Epoch 16/20\n",
      "951/951 [==============================] - 12s 12ms/step - loss: 1.6435 - val_loss: 1.8187\n",
      "Epoch 17/20\n",
      "951/951 [==============================] - 12s 13ms/step - loss: 1.6353 - val_loss: 1.8147\n",
      "Epoch 18/20\n",
      "951/951 [==============================] - 12s 12ms/step - loss: 1.6279 - val_loss: 1.8107\n",
      "Epoch 19/20\n",
      "951/951 [==============================] - 12s 13ms/step - loss: 1.6212 - val_loss: 1.8070\n",
      "Epoch 20/20\n",
      "951/951 [==============================] - 12s 13ms/step - loss: 1.6150 - val_loss: 1.8039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21a6a9388e0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, epochs=20, batch_size=batch_size, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "\n",
      "GREMIO:\n",
      "Good morrow, neighbou\n",
      "--------------------\n",
      "?\n",
      "\n",
      "GREMIO:\n",
      "Good morrow, neighbour!\n",
      "\n",
      "BUUNT:\n",
      "All of at, that you'll even stland-siscentless all be pustantent and an apbanise,\n",
      "With hi\n"
     ]
    }
   ],
   "source": [
    "for context, _ in val_ds.take(1):    \n",
    "    print(decode(context[0].numpy()))\n",
    "    result = generate(model, block_size , context, new_tokens)\n",
    "    print(\"-\" * 20)\n",
    "    print(decode(result.numpy()[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU - large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 64\n",
    "batch_size = 32\n",
    "n_embed = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = make_ds(text, block_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, n_embed, input_length=block_size),\n",
    "    tf.keras.layers.GRU(block_size * n_embed, return_sequences=True),\n",
    "    tf.keras.layers.Dense(vocab_size),\n",
    "    tf.keras.layers.Softmax(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.losses.SparseCategoricalCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_11 (Embedding)    (None, 64, 32)            2080      \n",
      "                                                                 \n",
      " gru_4 (GRU)                 (None, 64, 2048)          12791808  \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 64, 65)            133185    \n",
      "                                                                 \n",
      " softmax_10 (Softmax)        (None, 64, 65)            0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,927,073\n",
      "Trainable params: 12,927,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "483/483 [==============================] - 178s 365ms/step - loss: 2.4905 - val_loss: 2.0620\n",
      "Epoch 2/20\n",
      "483/483 [==============================] - 176s 364ms/step - loss: 1.8278 - val_loss: 1.7955\n",
      "Epoch 3/20\n",
      "483/483 [==============================] - 176s 364ms/step - loss: 1.5843 - val_loss: 1.6771\n",
      "Epoch 4/20\n",
      "483/483 [==============================] - 176s 364ms/step - loss: 1.4433 - val_loss: 1.6259\n",
      "Epoch 5/20\n",
      "483/483 [==============================] - 176s 364ms/step - loss: 1.3394 - val_loss: 1.6185\n",
      "Epoch 6/20\n",
      "483/483 [==============================] - 176s 364ms/step - loss: 1.2361 - val_loss: 1.6525\n",
      "Epoch 7/20\n",
      "483/483 [==============================] - 176s 364ms/step - loss: 1.1130 - val_loss: 1.7208\n",
      "Epoch 8/20\n",
      "483/483 [==============================] - 176s 364ms/step - loss: 0.9819 - val_loss: 1.8153\n",
      "Epoch 9/20\n",
      "483/483 [==============================] - 176s 364ms/step - loss: 0.8859 - val_loss: 1.9057\n",
      "Epoch 10/20\n",
      "483/483 [==============================] - 176s 364ms/step - loss: 0.8192 - val_loss: 1.9765\n",
      "Epoch 11/20\n",
      "483/483 [==============================] - 176s 364ms/step - loss: 0.7660 - val_loss: 2.0603\n",
      "Epoch 12/20\n",
      "483/483 [==============================] - 176s 364ms/step - loss: 0.7225 - val_loss: 2.1561\n",
      "Epoch 13/20\n",
      "483/483 [==============================] - 176s 364ms/step - loss: 0.6917 - val_loss: 2.2404\n",
      "Epoch 14/20\n",
      "483/483 [==============================] - 176s 364ms/step - loss: 0.6682 - val_loss: 2.2900\n",
      "Epoch 15/20\n",
      "483/483 [==============================] - 176s 364ms/step - loss: 0.6548 - val_loss: 2.3121\n",
      "Epoch 16/20\n",
      "483/483 [==============================] - 176s 364ms/step - loss: 0.6414 - val_loss: 2.3406\n",
      "Epoch 17/20\n",
      "483/483 [==============================] - 176s 364ms/step - loss: 0.6268 - val_loss: 2.3760\n",
      "Epoch 18/20\n",
      "483/483 [==============================] - 176s 365ms/step - loss: 0.6270 - val_loss: 2.4005\n",
      "Epoch 19/20\n",
      "483/483 [==============================] - 176s 364ms/step - loss: 0.6232 - val_loss: 2.4023\n",
      "Epoch 20/20\n",
      "483/483 [==============================] - 176s 364ms/step - loss: 0.6189 - val_loss: 2.4270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21a6fde83d0>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, epochs=20, batch_size=batch_size, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "\n",
      "GREMIO:\n",
      "Good morrow, neighbour Baptista.\n",
      "\n",
      "BAPTISTA:\n",
      "Good morr\n",
      "--------------------\n",
      "?\n",
      "\n",
      "GREMIO:\n",
      "Good morrow, neighbour Baptista.\n",
      "\n",
      "BAPTISTA:\n",
      "Good morrow to: it is as mine own\n",
      "So sairted so insid: a propage attain'd;\n",
      "I herred in the true gentleman to \n"
     ]
    }
   ],
   "source": [
    "for context, _ in val_ds.take(1):    \n",
    "    print(decode(context[0].numpy()))\n",
    "    result = generate(model, block_size , context, new_tokens)\n",
    "    print(\"-\" * 20)\n",
    "    print(decode(result.numpy()[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM - small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 32\n",
    "batch_size = 32\n",
    "n_embed = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = make_ds(text, block_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, n_embed, input_length=block_size),\n",
    "    tf.keras.layers.LSTM(block_size * n_embed, return_sequences=True),\n",
    "    tf.keras.layers.Dense(vocab_size),\n",
    "    tf.keras.layers.Softmax(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_12 (Embedding)    (None, 32, 4)             260       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 32, 128)           68096     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 32, 65)            8385      \n",
      "                                                                 \n",
      " softmax_11 (Softmax)        (None, 32, 65)            0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 76,741\n",
      "Trainable params: 76,741\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.losses.SparseCategoricalCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "951/951 [==============================] - 12s 11ms/step - loss: 2.9336 - val_loss: 2.5307\n",
      "Epoch 2/20\n",
      "951/951 [==============================] - 10s 10ms/step - loss: 2.4101 - val_loss: 2.3155\n",
      "Epoch 3/20\n",
      "951/951 [==============================] - 9s 10ms/step - loss: 2.2402 - val_loss: 2.1955\n",
      "Epoch 4/20\n",
      "951/951 [==============================] - 10s 11ms/step - loss: 2.1202 - val_loss: 2.1092\n",
      "Epoch 5/20\n",
      "951/951 [==============================] - 10s 11ms/step - loss: 2.0281 - val_loss: 2.0444\n",
      "Epoch 6/20\n",
      "951/951 [==============================] - 10s 11ms/step - loss: 1.9577 - val_loss: 1.9972\n",
      "Epoch 7/20\n",
      "951/951 [==============================] - 10s 11ms/step - loss: 1.9026 - val_loss: 1.9602\n",
      "Epoch 8/20\n",
      "951/951 [==============================] - 10s 11ms/step - loss: 1.8584 - val_loss: 1.9319\n",
      "Epoch 9/20\n",
      "951/951 [==============================] - 10s 10ms/step - loss: 1.8217 - val_loss: 1.9100\n",
      "Epoch 10/20\n",
      "951/951 [==============================] - 10s 11ms/step - loss: 1.7911 - val_loss: 1.8902\n",
      "Epoch 11/20\n",
      "951/951 [==============================] - 10s 11ms/step - loss: 1.7650 - val_loss: 1.8731\n",
      "Epoch 12/20\n",
      "951/951 [==============================] - 10s 11ms/step - loss: 1.7424 - val_loss: 1.8585\n",
      "Epoch 13/20\n",
      "951/951 [==============================] - 9s 10ms/step - loss: 1.7227 - val_loss: 1.8455\n",
      "Epoch 14/20\n",
      "951/951 [==============================] - 10s 10ms/step - loss: 1.7054 - val_loss: 1.8339\n",
      "Epoch 15/20\n",
      "951/951 [==============================] - 10s 11ms/step - loss: 1.6901 - val_loss: 1.8237\n",
      "Epoch 16/20\n",
      "951/951 [==============================] - 10s 11ms/step - loss: 1.6766 - val_loss: 1.8148\n",
      "Epoch 17/20\n",
      "951/951 [==============================] - 10s 11ms/step - loss: 1.6644 - val_loss: 1.8074\n",
      "Epoch 18/20\n",
      "951/951 [==============================] - 10s 11ms/step - loss: 1.6534 - val_loss: 1.8007\n",
      "Epoch 19/20\n",
      "951/951 [==============================] - 10s 11ms/step - loss: 1.6435 - val_loss: 1.7947\n",
      "Epoch 20/20\n",
      "951/951 [==============================] - 10s 10ms/step - loss: 1.6344 - val_loss: 1.7893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21a8852ee00>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, epochs=20, batch_size=batch_size, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "\n",
      "GREMIO:\n",
      "Good morrow, neighbou\n",
      "--------------------\n",
      "?\n",
      "\n",
      "GREMIO:\n",
      "Good morrow, neighboused of it in a wine;\n",
      "That the handsables, sir. Master by the not out name, thereforal, if I would of\n"
     ]
    }
   ],
   "source": [
    "for context, _ in val_ds.take(1):    \n",
    "    print(decode(context[0].numpy()))\n",
    "    result = generate(model, block_size , context, new_tokens)\n",
    "    print(\"-\" * 20)\n",
    "    print(decode(result.numpy()[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM - large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 64\n",
    "batch_size = 32\n",
    "n_embed = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = make_ds(text, block_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, n_embed, input_length=block_size),\n",
    "    tf.keras.layers.LSTM(block_size * n_embed, return_sequences=True),\n",
    "    tf.keras.layers.Dense(vocab_size),\n",
    "    tf.keras.layers.Softmax(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_13 (Embedding)    (None, 64, 32)            2080      \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64, 2048)          17047552  \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 64, 65)            133185    \n",
      "                                                                 \n",
      " softmax_12 (Softmax)        (None, 64, 65)            0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,182,817\n",
      "Trainable params: 17,182,817\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.losses.SparseCategoricalCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "483/483 [==============================] - 221s 455ms/step - loss: 2.5800 - val_loss: 2.0928\n",
      "Epoch 2/20\n",
      "483/483 [==============================] - 219s 454ms/step - loss: 1.8458 - val_loss: 1.7905\n",
      "Epoch 3/20\n",
      "483/483 [==============================] - 219s 454ms/step - loss: 1.5724 - val_loss: 1.6470\n",
      "Epoch 4/20\n",
      "483/483 [==============================] - 219s 454ms/step - loss: 1.4174 - val_loss: 1.5995\n",
      "Epoch 5/20\n",
      "483/483 [==============================] - 219s 454ms/step - loss: 1.3045 - val_loss: 1.5971\n",
      "Epoch 6/20\n",
      "483/483 [==============================] - 219s 454ms/step - loss: 1.1907 - val_loss: 1.6443\n",
      "Epoch 7/20\n",
      "483/483 [==============================] - 219s 454ms/step - loss: 1.0670 - val_loss: 1.7194\n",
      "Epoch 8/20\n",
      "483/483 [==============================] - 219s 454ms/step - loss: 0.9685 - val_loss: 1.7986\n",
      "Epoch 9/20\n",
      "483/483 [==============================] - 219s 454ms/step - loss: 0.8961 - val_loss: 1.8494\n",
      "Epoch 10/20\n",
      "483/483 [==============================] - 219s 454ms/step - loss: 0.8209 - val_loss: 1.9177\n",
      "Epoch 11/20\n",
      "483/483 [==============================] - 219s 454ms/step - loss: 0.7574 - val_loss: 2.0034\n",
      "Epoch 12/20\n",
      "483/483 [==============================] - 219s 454ms/step - loss: 0.7083 - val_loss: 2.0589\n",
      "Epoch 13/20\n",
      "483/483 [==============================] - 219s 454ms/step - loss: 0.6667 - val_loss: 2.1121\n",
      "Epoch 14/20\n",
      "483/483 [==============================] - 219s 454ms/step - loss: 0.6379 - val_loss: 2.1652\n",
      "Epoch 15/20\n",
      "483/483 [==============================] - 219s 454ms/step - loss: 0.6099 - val_loss: 2.2120\n",
      "Epoch 16/20\n",
      "483/483 [==============================] - 219s 454ms/step - loss: 0.5865 - val_loss: 2.2663\n",
      "Epoch 17/20\n",
      "483/483 [==============================] - 219s 454ms/step - loss: 0.5688 - val_loss: 2.2937\n",
      "Epoch 18/20\n",
      "483/483 [==============================] - 219s 454ms/step - loss: 0.5572 - val_loss: 2.3322\n",
      "Epoch 19/20\n",
      "483/483 [==============================] - 219s 454ms/step - loss: 0.5448 - val_loss: 2.3631\n",
      "Epoch 20/20\n",
      "483/483 [==============================] - 219s 454ms/step - loss: 0.5338 - val_loss: 2.3941\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21ad7708250>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, epochs=20, batch_size=batch_size, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "\n",
      "GREMIO:\n",
      "Good morrow, neighbour Baptista.\n",
      "\n",
      "BAPTISTA:\n",
      "Good morr\n",
      "--------------------\n",
      "?\n",
      "\n",
      "GREMIO:\n",
      "Good morrow, neighbour Baptista.\n",
      "\n",
      "BAPTISTA:\n",
      "Good morrow, sir, I see, you would not speak with honour fair.\n",
      "Say you so protect, my gifts talk thou wilv bu\n"
     ]
    }
   ],
   "source": [
    "for context, _ in val_ds.take(1):    \n",
    "    print(decode(context[0].numpy()))\n",
    "    result = generate(model, block_size , context, new_tokens)\n",
    "    print(\"-\" * 20)\n",
    "    print(decode(result.numpy()[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 256\n",
    "batch_size = 32\n",
    "n_embed = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = make_ds(text, block_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, n_embed, input_length=block_size),\n",
    "    tf.keras.layers.LSTM(2048, return_sequences=True),\n",
    "    tf.keras.layers.LayerNormalization(),\n",
    "    tf.keras.layers.Dense(vocab_size),\n",
    "    tf.keras.layers.Softmax(),\n",
    "    tf.keras.layers.Dropout(0.1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 256, 16)           1040      \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 256, 2048)         16916480  \n",
      "                                                                 \n",
      " layer_normalization_1 (Laye  (None, 256, 2048)        4096      \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256, 65)           133185    \n",
      "                                                                 \n",
      " softmax_2 (Softmax)         (None, 256, 65)           0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256, 65)           0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,054,801\n",
      "Trainable params: 17,054,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.losses.SparseCategoricalCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "123/123 [==============================] - 73s 570ms/step - loss: 6.3518 - val_loss: 3.4023\n",
      "Epoch 2/20\n",
      "123/123 [==============================] - 86s 702ms/step - loss: 4.5225 - val_loss: 3.3658\n",
      "Epoch 3/20\n",
      "123/123 [==============================] - 89s 724ms/step - loss: 4.2851 - val_loss: 2.9228\n",
      "Epoch 4/20\n",
      "123/123 [==============================] - 89s 725ms/step - loss: 3.8577 - val_loss: 2.5197\n",
      "Epoch 5/20\n",
      "123/123 [==============================] - 100s 812ms/step - loss: 3.6847 - val_loss: 2.4012\n",
      "Epoch 6/20\n",
      "123/123 [==============================] - 79s 639ms/step - loss: 3.6084 - val_loss: 2.3381\n",
      "Epoch 7/20\n",
      "123/123 [==============================] - 86s 705ms/step - loss: 3.5358 - val_loss: 2.3062\n",
      "Epoch 8/20\n",
      "123/123 [==============================] - 90s 730ms/step - loss: 3.4840 - val_loss: 2.1985\n",
      "Epoch 9/20\n",
      "123/123 [==============================] - 97s 790ms/step - loss: 3.4006 - val_loss: 2.1461\n",
      "Epoch 10/20\n",
      "123/123 [==============================] - 98s 800ms/step - loss: 3.3315 - val_loss: 2.1119\n",
      "Epoch 11/20\n",
      "123/123 [==============================] - 103s 840ms/step - loss: 3.2733 - val_loss: 2.0403\n",
      "Epoch 12/20\n",
      "123/123 [==============================] - 86s 705ms/step - loss: 3.2015 - val_loss: 1.9955\n",
      "Epoch 13/20\n",
      "123/123 [==============================] - 87s 709ms/step - loss: 3.1313 - val_loss: 1.9696\n",
      "Epoch 14/20\n",
      "123/123 [==============================] - 95s 774ms/step - loss: 3.0704 - val_loss: 1.9441\n",
      "Epoch 15/20\n",
      "123/123 [==============================] - 96s 779ms/step - loss: 3.0217 - val_loss: 1.9125\n",
      "Epoch 16/20\n",
      "123/123 [==============================] - 89s 728ms/step - loss: 2.9644 - val_loss: 1.8817\n",
      "Epoch 17/20\n",
      "123/123 [==============================] - 92s 750ms/step - loss: 2.9115 - val_loss: 1.8338\n",
      "Epoch 18/20\n",
      "123/123 [==============================] - 84s 685ms/step - loss: 2.8557 - val_loss: 1.8184\n",
      "Epoch 19/20\n",
      "123/123 [==============================] - 92s 686ms/step - loss: 2.8114 - val_loss: 1.7984\n",
      "Epoch 20/20\n",
      "123/123 [==============================] - 99s 808ms/step - loss: 2.7621 - val_loss: 1.7869\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b8f93307f0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, epochs=20, batch_size=batch_size, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "\n",
      "GREMIO:\n",
      "Good morrow, neighbour Baptista.\n",
      "\n",
      "BAPTISTA:\n",
      "Good morrow, neighbour Gremio.\n",
      "God save you, gentlemen!\n",
      "\n",
      "PETRUCHIO:\n",
      "And you, good sir! Pray, have you not a daughter\n",
      "Call'd Katharina, fair and virtuous?\n",
      "\n",
      "BAPTISTA:\n",
      "I have a daughter, sir, called Katha\n",
      "--------------------\n",
      "?\n",
      "\n",
      "GREMIO:\n",
      "Good morrow, neighbour Baptista.\n",
      "\n",
      "BAPTISTA:\n",
      "Good morrow, neighbour Gremio.\n",
      "God save you, gentlemen!\n",
      "\n",
      "PETRUCHIO:\n",
      "And you, good sir! Pray, have you not a daughter\n",
      "Call'd Katharina, fair and virtuous?\n",
      "\n",
      "BAPTISTA:\n",
      "I have a daughter, sir, called Kathan that tell thee,\n",
      "And your cozer of your geest.\n",
      "\n",
      "ANGELO:\n",
      "And I'll will mon well never find contentence:\n",
      "And not your revenge with comeen their least\n",
      "when shall begawe the tears woman seak madestable\n",
      "A lengled hath it speed  toonge, became thee us?\n",
      "\n",
      "GLOUCESTER:\n",
      "I think you are can from thee when you been your hoar,\n",
      "Till eeot eyes faith, on my woman's sirder;\n",
      "I knee thee in peice: bit is fallestle, And even seech\n",
      "Sisterners you? what teil? Frield, say then all I said after a\n",
      "woman, as I fiild away\n"
     ]
    }
   ],
   "source": [
    "for context, _ in val_ds.take(1):    \n",
    "    print(decode(context[0].numpy()))\n",
    "    result = generate(model, block_size , context, 5 * new_tokens)\n",
    "    print(\"-\" * 20)\n",
    "    print(decode(result.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "\n",
      "GREMIO:\n",
      "Good morrow, neighbour Baptista.\n",
      "\n",
      "BAPTISTA:\n",
      "Good morrow, neighbour Gremio.\n",
      "God save you, gentlemen!\n",
      "\n",
      "PETRUCHIO:\n",
      "And you, good sir! Pray, have you not a daughter\n",
      "Call'd Katharina, fair and virtuous?\n",
      "\n",
      "BAPTISTA:\n",
      "I have a daughter, sir, called Kathan that tell thee,\n",
      "And your cozer of your geest.\n",
      "\n",
      "ANGELO:\n",
      "And I'll will mon well never find contentence:\n",
      "And not your revenge with comeen their least\n",
      "when shall begawe the tears woman seak madestable\n",
      "A lengled hath it speed  toonge, became thee us?\n",
      "\n",
      "GLOUCESTER:\n",
      "I think you are can from thee when you been your hoar,\n",
      "Till eeot eyes faith, on my woman's sirder;\n",
      "I knee thee in peice: bit is fallestle, And even seech\n",
      "Sisterners you? what teil? Frield, say then all I said after a\n",
      "woman, as I fiild away\n"
     ]
    }
   ],
   "source": [
    "print(decode(result.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10256"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = generate(model, block_size , context, 10000)\n",
    "open('more.txt', 'w').write(decode(result.numpy()[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final version in model.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
